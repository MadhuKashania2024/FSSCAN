{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d3086c1-3307-44ea-b52f-dc7d2c2b6234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81dc73ae-e47b-4675-9022-29397467d174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "og_mask shape: (1, 1, 320, 1)\n"
     ]
    }
   ],
   "source": [
    "mask = np.load(r\"C:\\Users\\DU\\aman_fastmri\\Data\\mask_4x_320_random.npy\")  # Shape: (1, 320, 320)\n",
    "#C:\\Users\\DU\\aman_fastmri\\Data\n",
    "print(\"og_mask shape:\", mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4ec03e6-b795-4011-a034-3dcee27e20d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run model.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2de1d09b-fb03-44ef-81c8-4eb9dae8f64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RSCAGAN(in_channels=2, base_channels=32)\n",
    "generator = model.generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "75dfc105-3f6c-4bf6-88d1-c024d07be1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Generator weights restored\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Paths\n",
    "# --------------------------------------------------\n",
    "SAVE_DIR = \"./SavedModels_RSCA_GAN_full_2\"\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Build model (EXACT SAME)\n",
    "# --------------------------------------------------\n",
    "model = RSCAGAN(in_channels=2, base_channels=32)\n",
    "generator = model.generator\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Restore checkpoint\n",
    "# --------------------------------------------------\n",
    "ckpt = tf.train.Checkpoint(generator=generator)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(\n",
    "    ckpt,\n",
    "    directory=SAVE_DIR,\n",
    "    max_to_keep=5\n",
    ")\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint).expect_partial()\n",
    "    print(\"✅ Generator weights restored\")\n",
    "else:\n",
    "    raise RuntimeError(\"❌ No checkpoint found for inference\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7fba0d75-9ef3-4252-b614-c3cf1b57b53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class MRISliceValGenerator(tf.keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, file_list, batch_size=4, shuffle=False):\n",
    "        self.file_list = file_list\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        # Open files once\n",
    "        self.files = [h5py.File(fp, 'r') for fp in self.file_list]\n",
    "\n",
    "        self.slice_index_map = []\n",
    "        self._build_index()\n",
    "\n",
    "    def _build_index(self):\n",
    "        for file_idx, f in enumerate(self.files):\n",
    "            num_slices = f['image_under'].shape[0]\n",
    "            for slice_idx in range(num_slices):\n",
    "                self.slice_index_map.append((file_idx, slice_idx))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.slice_index_map) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "    \n",
    "        batch_map = self.slice_index_map[\n",
    "            index * self.batch_size:(index + 1) * self.batch_size\n",
    "        ]\n",
    "    \n",
    "        SZF_batch = []\n",
    "        SGT_batch = []\n",
    "        MAXV_batch = []\n",
    "        VOL_batch = []\n",
    "        SLICE_batch = []\n",
    "    \n",
    "        for file_idx, slice_idx in batch_map:\n",
    "            f = self.files[file_idx]\n",
    "    \n",
    "            SZF_batch.append(f['image_under'][slice_idx])\n",
    "            SGT_batch.append(f['image_full'][slice_idx])\n",
    "            MAXV_batch.append(f['max_val_full_image'][0])\n",
    "    \n",
    "            VOL_batch.append(file_idx)\n",
    "            SLICE_batch.append(slice_idx)\n",
    "    \n",
    "        return (\n",
    "            np.stack(SZF_batch).astype(np.float32),\n",
    "            np.stack(SGT_batch).astype(np.float32),\n",
    "            np.array(MAXV_batch).astype(np.float32),\n",
    "            np.array(VOL_batch),\n",
    "            np.array(SLICE_batch),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29e32596-9a49-4fb6-be04-205e59b321af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7135\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "train_folder = r\"D:\\fastmri_singlecoil_FSSCAN\\train_norm\"\n",
    "val_folder = r\"D:\\fastmri_singlecoil_FSSCAN\\val_norm\"\n",
    "kspace_files_list_val = sorted(glob.glob(os.path.join(val_folder, \"*.h5\")))\n",
    "\n",
    "# half_train = 20\n",
    "# half_val = 10\n",
    " \n",
    "half_val = len(kspace_files_list_val) \n",
    "# print(\"half_train\",half_train)\n",
    "# print(\"half_val\",half_val)\n",
    "kspace_files_list_val = kspace_files_list_val[:]\n",
    "\n",
    "# # Create generators\n",
    "# train_gen = MRISliceGenerator(kspace_files_list_train,batch_size=16, shuffle=True,mask=mask)\n",
    "# val_gen = MRISliceGenerator(kspace_files_list_val, batch_size=4, shuffle=False,mask=mask)\n",
    "\n",
    "val_gen = MRISliceValGenerator(kspace_files_list_val, batch_size=1, shuffle=False)\n",
    "\n",
    " \n",
    "print(len(val_gen))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "51f8d87f-6a7b-434f-b24c-0af210d5c256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_to_mag(x):\n",
    "    return tf.sqrt(x[..., 0]**2 + x[..., 1]**2 + 1e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b73f5018-f3cc-41bb-98e4-eb257556802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def inference_metrics_step_slice(SZF, SGT, MAXV):\n",
    "\n",
    "    SRE = generator(SZF, training=False)\n",
    "\n",
    "    gt_mag  = complex_to_mag(SGT)\n",
    "    rec_mag = complex_to_mag(SRE)\n",
    "\n",
    "    MAXV = tf.reshape(MAXV, (-1, 1, 1))\n",
    "    gt_mag  = gt_mag * MAXV\n",
    "    rec_mag = rec_mag * MAXV\n",
    "\n",
    "    gt_mag  = tf.expand_dims(gt_mag, axis=-1)\n",
    "    rec_mag = tf.expand_dims(rec_mag, axis=-1)\n",
    "\n",
    "    ssim = tf.image.ssim(gt_mag, rec_mag, max_val=MAXV)\n",
    "\n",
    "    # PSNR and NMSE are still useful for volume aggregation\n",
    "    psnr = tf.image.psnr(gt_mag, rec_mag, max_val=MAXV)\n",
    "\n",
    "    nmse = tf.reduce_sum(\n",
    "        tf.square(gt_mag - rec_mag), axis=[1,2,3]\n",
    "    ) / tf.reduce_sum(\n",
    "        tf.square(gt_mag), axis=[1,2,3]\n",
    "    )\n",
    "\n",
    "    return psnr, ssim, nmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1137dc08-fcaa-43a3-ae29-1bd58ae1b19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "def run_evaluation_and_save(val_gen):\n",
    "\n",
    "    # -----------------------\n",
    "    # Storage\n",
    "    # -----------------------\n",
    "    volume_psnr = defaultdict(list)\n",
    "    volume_nmse = defaultdict(list)\n",
    "\n",
    "    slice_ssim_records = []\n",
    "\n",
    "    # -----------------------\n",
    "    # Loop\n",
    "    # -----------------------\n",
    "    for SZF, SGT, MAXV, VOL, SLICE in val_gen:\n",
    "\n",
    "        psnr, ssim, nmse = inference_metrics_step_slice(\n",
    "            SZF, SGT, MAXV\n",
    "        )\n",
    "\n",
    "        psnr = psnr.numpy()\n",
    "        ssim = ssim.numpy()\n",
    "        nmse = nmse.numpy()\n",
    "\n",
    "        for i in range(len(VOL)):\n",
    "            vol_id   = int(VOL[i])\n",
    "            slice_id = int(SLICE[i])\n",
    "\n",
    "            volume_psnr[vol_id].append(psnr[i])\n",
    "            volume_nmse[vol_id].append(nmse[i])\n",
    "\n",
    "            slice_ssim_records.append({\n",
    "                \"volume_id\": vol_id,\n",
    "                \"slice_id\": slice_id,\n",
    "                \"SSIM\": float(ssim[i])\n",
    "            })\n",
    "\n",
    "    # -----------------------\n",
    "    # Volume-wise aggregation\n",
    "    # -----------------------\n",
    "    volume_records = []\n",
    "\n",
    "    for vol_id in volume_psnr.keys():\n",
    "        volume_records.append({\n",
    "            \"volume_id\": vol_id,\n",
    "            \"PSNR\": np.mean(volume_psnr[vol_id]),\n",
    "            \"NMSE\": np.mean(volume_nmse[vol_id])\n",
    "        })\n",
    "\n",
    "    # -----------------------\n",
    "    # Save to Excel\n",
    "    # -----------------------\n",
    "    df_volume = pd.DataFrame(volume_records)\n",
    "    df_slice  = pd.DataFrame(slice_ssim_records)\n",
    "\n",
    "    df_volume.to_excel(\n",
    "        \"volume_wise_PSNR_NMSE.xlsx\",\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    df_slice.to_excel(\n",
    "        \"slice_wise_SSIM.xlsx\",\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    print(\"✅ Saved:\")\n",
    "    print(\"  - volume_wise_PSNR_NMSE.xlsx\")\n",
    "    print(\"  - slice_wise_SSIM.xlsx\")\n",
    "\n",
    "    return df_volume, df_slice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0d1402a7-3115-4fba-a4fc-4f18a68c9bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved:\n",
      "  - volume_wise_PSNR_NMSE.xlsx\n",
      "  - slice_wise_SSIM.xlsx\n"
     ]
    }
   ],
   "source": [
    "df_volume, df_slice = run_evaluation_and_save(val_gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b9331b-3379-4059-9432-372e564ea23a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1b35ec-37bb-46c0-9688-6a1489b0fa66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
