{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "547fde1c-5b75-4c1e-8101-62d377a83ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0e72c78-a715-4766-965c-a31c39bcfb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveGlobalFilter(tf.keras.layers.Layer):\n",
    "    def __init__(self, ratio=10):\n",
    "        super().__init__()\n",
    "        self.ratio = ratio\n",
    "        # self.filter  # [C, H, W, 2]\n",
    "\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # input_shape: [B, H, W, C, 2]\n",
    "        _, H, W, C, _ = input_shape\n",
    "        self.H, self.W, self.C = H, W, C\n",
    "\n",
    "        # initializer = tf.keras.initializers.RandomNormal(stddev=0.02)\n",
    "        self.filter = self.add_weight(\n",
    "            shape=(C, H, W, 2),\n",
    "            # initializer=initializer,\n",
    "            trainable=True,\n",
    "            name='complex_filter'\n",
    "        )\n",
    "\n",
    "        self.mask_low = self._create_static_mask()\n",
    "        self.mask_high = 1.0 - self.mask_low\n",
    "\n",
    "    def _create_static_mask(self):\n",
    "        mask = tf.zeros((self.H, self.W), dtype=tf.float32)\n",
    "        crow, ccol = self.H // 2, self.W // 2\n",
    "        r = self.ratio\n",
    "\n",
    "        indices = tf.stack(tf.meshgrid(\n",
    "            tf.range(crow - r, crow + r),\n",
    "            tf.range(ccol - r, ccol + r),\n",
    "            indexing='ij'\n",
    "        ), axis=-1)\n",
    "        indices = tf.reshape(indices, [-1, 2])\n",
    "        updates = tf.ones((tf.shape(indices)[0],), dtype=tf.float32)\n",
    "\n",
    "        return tf.Variable(tf.tensor_scatter_nd_update(mask, indices, updates), trainable=False)\n",
    "\n",
    "    def call(self, x):\n",
    "        tf.debugging.check_numerics(x, \"Input to AGF has NaNs/Infs\")\n",
    "        # x: [B, H, W, C, 2]\n",
    "        x = tf.transpose(x, [0, 3, 1, 2, 4])  # [B, C, H, W, 2]\n",
    "        x_freq = fft2c_tf(x)                 # [B, C, H, W, 2]\n",
    "        tf.debugging.check_numerics(x_freq, \"FFT2 output has NaNs/Infs\")\n",
    "\n",
    "\n",
    "        # Convert learnable filter to complex\n",
    "        real_part = self.filter[..., 0]\n",
    "        imag_part = self.filter[..., 1]\n",
    "        weight = tf.complex(real_part, imag_part)  # [C, H, W]\n",
    "        weight = tf.expand_dims(weight, axis=0)    # [1, C, H, W]\n",
    "\n",
    "        # Cast masks\n",
    "        mask_low_c = tf.cast(tf.reshape(self.mask_low, [1, 1, self.H, self.W]), tf.complex64)\n",
    "        mask_high_c = tf.cast(tf.reshape(self.mask_high, [1, 1, self.H, self.W]), tf.complex64)\n",
    "\n",
    "        # Convert x_freq to complex\n",
    "        x_freq_c = tf.complex(x_freq[..., 0], x_freq[..., 1])  # [B, C, H, W]\n",
    "\n",
    "        x_low = x_freq_c * mask_low_c\n",
    "        x_high = x_freq_c * mask_high_c\n",
    "\n",
    "        x_low_filtered = x_low * weight\n",
    "        x_combined = x_low_filtered + x_high  # [B, C, H, W]\n",
    "\n",
    "        x_combined = tf.stack([tf.math.real(x_combined), tf.math.imag(x_combined)], axis=-1)  # [B, C, H, W, 2]\n",
    "        x_combined = tf.transpose(x_combined, [0, 2, 3, 1, 4])  # [B, H, W, C, 2]\n",
    "\n",
    "        # Apply IFFT\n",
    "        x_out = ifft2c_tf(x_combined)  # [B, H, W, C, 2]\n",
    "        tf.debugging.check_numerics(x_out, \"IFFT2 output has NaNs/Infs\")\n",
    "        return x_out\n",
    "\n",
    "# # # Example dummy input: [batch, height, width, channels]\n",
    "# dummy_input = tf.random.normal((2, 512, 512, 64,2))\n",
    "\n",
    "# # Instantiate layer\n",
    "# agf = AdaptiveGlobalFilter(ratio=10)\n",
    "\n",
    "# # Forward pass\n",
    "# output = agf(dummy_input)\n",
    "\n",
    "# print(\"Input shape: \", dummy_input.shape)\n",
    "# print(\"Output shape:\", output.shape)Under non-Cartesian or aggressive masks:\n",
    "\n",
    "#Frequency separation may be suboptimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae78d93d-64ed-460f-8467-78de46ad8363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input  shape: (16, 128, 128, 16, 2)\n",
      "Output shape: (16, 128, 128, 16, 2)\n"
     ]
    }
   ],
   "source": [
    "class SpatialAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv = tf.keras.layers.Conv2D(1, kernel_size=7, padding='same', use_bias=False)\n",
    "        self.sigmoid = tf.keras.activations.sigmoid\n",
    "        self.last_attn_map = None\n",
    "\n",
    "    def call(self, x, return_attn=False):\n",
    "        real = x[..., 0]\n",
    "        imag = x[..., 1]\n",
    "        mag = tf.sqrt(real**2 + imag**2 + 1e-6)\n",
    "        avg_out = tf.reduce_mean(mag, axis=-1, keepdims=True)\n",
    "        max_out = tf.reduce_max(mag, axis=-1, keepdims=True)\n",
    "        concat = tf.concat([avg_out, max_out], axis=-1)\n",
    "        attn_map = self.sigmoid(self.conv(concat))\n",
    "        attn_map = tf.expand_dims(attn_map, axis=-1)\n",
    "\n",
    "        self.last_attn_map = attn_map  # Save for visualization\n",
    "\n",
    "        return x * attn_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b176933e-c4ec-47eb-9875-b498ad077acd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7411ccd9-7db0-4dc0-bd3d-b99b73fe804c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FSA(tf.keras.layers.Layer):\n",
    "    def __init__(self, ratio=10):\n",
    "        super().__init__()\n",
    "        self.agf = AdaptiveGlobalFilter(ratio=ratio)\n",
    "        self.sa = SpatialAttention()\n",
    "\n",
    "\n",
    "    def call(self, x, return_attn=False):\n",
    "        out1 = self.agf(x)\n",
    "        if return_attn:\n",
    "            out2, attn_map = self.sa(x, return_attn=True)\n",
    "            out = out1 + out2\n",
    "            return out, attn_map\n",
    "        else:\n",
    "            out2 = self.sa(x)\n",
    "            out = out1 + out2\n",
    "            return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e2478ee-cac5-45f5-bdf9-45fc7c83326f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipConnection(tf.keras.layers.Layer):\n",
    "    def __init__(self, ratio, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.fsa = FSA(ratio=ratio)\n",
    "\n",
    "    def call(self, real, imag):\n",
    "        \"\"\"\n",
    "        real, imag: Tensors of shape [B, H, W, C]\n",
    "        \"\"\"\n",
    "        # # Stack real and imag along the last axis -> [B, H, W, C, 2]\n",
    "        x = tf.stack([real, imag], axis=-1)\n",
    "        tf.debugging.check_numerics(x, \"üö® FSA input has NaNs/Infs\")\n",
    "\n",
    "        # tf.print(\"x shape:\", tf.shape(x))\n",
    "\n",
    "        # # Apply FSA block\n",
    "        y = self.fsa(x)  # [B, H, W, C, 2]\n",
    "        # tf.debugging.check_numerics(y, \"üö® FSA output has NaNs/Infs\")\n",
    "        \n",
    "\n",
    "        # # Split output back to real and imaginary parts\n",
    "        return y[..., 0], y[..., 1]\n",
    "\n",
    "        # return real,imag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f616e04-5902-4c4a-9538-147730d50040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, Lambda, Add, LeakyReLU,  \\\n",
    "                                    MaxPooling2D, concatenate, UpSampling2D,\\\n",
    "                                    Multiply, ZeroPadding2D, Cropping2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22c6096-7e1b-45db-b317-3f45e4bb8a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"C:\\\\Users\\\\DU\\\\aman_fastmri\\\\fft.ipynb\"\n",
    "    \n",
    "%run \"C:\\\\Users\\\\DU\\\\aman_fastmri\\\\activation.ipynb\"\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743609fc-a37f-4d71-babd-9e528f65aa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"C:\\\\Users\\\\DU\\\\aman_fastmri\\\\layer.ipynb\"\n",
    "\n",
    "# %run layer.ipynb\n",
    "# %run activation.ipynb\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bab4f92f-d6ee-4ecc-9c0f-84ed718cab49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "class ComplexEncoderTF(Model):\n",
    "    def __init__(self, input_shape=(320, 320, 2)):\n",
    "        super(ComplexEncoderTF, self).__init__()\n",
    "\n",
    "        # Block 1\n",
    "        self.block1_conv1 = complex_Conv2D(64, kernel_size=3, padding='same')\n",
    "        self.block1_conv2 = complex_Conv2D(64, kernel_size=3, padding='same')\n",
    "        self.pool1 = ComplexMaxPool2D_mag(pool_size=(2, 2))\n",
    "\n",
    "        # Block 2\n",
    "        self.block2_conv1 = complex_Conv2D(128, kernel_size=3, padding='same')\n",
    "        self.block2_conv2 = complex_Conv2D(128, kernel_size=3, padding='same')\n",
    "        self.pool2 = ComplexMaxPool2D_mag(pool_size=(2, 2))\n",
    "\n",
    "        # Block 3\n",
    "        self.block3_conv1 = complex_Conv2D(256, kernel_size=3, padding='same')\n",
    "        self.block3_conv2 = complex_Conv2D(256, kernel_size=3, padding='same')\n",
    "        self.pool3 = ComplexMaxPool2D_mag(pool_size=(2, 2))\n",
    "\n",
    "        # Block 4\n",
    "        self.block4_conv1 = complex_Conv2D(512, kernel_size=3, padding='same')\n",
    "        self.block4_conv2 = complex_Conv2D(512, kernel_size=3, padding='same')\n",
    "        self.pool4 = ComplexMaxPool2D_mag(pool_size=(2, 2))\n",
    "\n",
    "        # Block 5 (no pooling)\n",
    "        self.block5_conv1 = complex_Conv2D(512, kernel_size=3, padding='same')\n",
    "        self.block5_conv2 = complex_Conv2D(512, kernel_size=3, padding='same')\n",
    "\n",
    "        self.build((None, *input_shape))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        real = tf.expand_dims(inputs[..., 0], axis=-1)\n",
    "        imag = tf.expand_dims(inputs[..., 1], axis=-1)\n",
    "\n",
    "        # Block 1\n",
    "        r, i = self.block1_conv1(real, imag)\n",
    "        r, i = CReLU(r, i)\n",
    "        r, i = self.block1_conv2(r, i)\n",
    "        r, i = CReLU(r, i)\n",
    "        feat1 = (r, i)\n",
    "        r, i = self.pool1(r, i)\n",
    "\n",
    "        # Block 2\n",
    "        r, i = self.block2_conv1(r, i)\n",
    "        r, i = CReLU(r, i)\n",
    "        r, i = self.block2_conv2(r, i)\n",
    "        r, i = CReLU(r, i)\n",
    "        feat2 = (r, i)\n",
    "        r, i = self.pool2(r, i)\n",
    "\n",
    "        # Block 3\n",
    "        r, i = self.block3_conv1(r, i)\n",
    "        r, i = CReLU(r, i)\n",
    "        r, i = self.block3_conv2(r, i)\n",
    "        r, i = CReLU(r, i)\n",
    "        feat3 = (r, i)\n",
    "        r, i = self.pool3(r, i)\n",
    "\n",
    "        # Block 4\n",
    "        r, i = self.block4_conv1(r, i)\n",
    "        r, i = CReLU(r, i)\n",
    "        r, i = self.block4_conv2(r, i)\n",
    "        r, i = CReLU(r, i)\n",
    "        feat4 = (r, i)\n",
    "        r, i = self.pool4(r, i)\n",
    "\n",
    "        # Block 5 (no pooling)\n",
    "        r, i = self.block5_conv1(r, i)\n",
    "        r, i = CReLU(r, i)\n",
    "        r, i = self.block5_conv2(r, i)\n",
    "        r, i = CReLU(r, i)\n",
    "        feat5 = (r, i)\n",
    "\n",
    "        return feat1, feat2, feat3, feat4, feat5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6925e9b6-12cf-4af5-b7bd-7d750f8542b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexUnetUpTF(tf.keras.layers.Layer):\n",
    "    def __init__(self, out_channels):\n",
    "        super(ComplexUnetUpTF, self).__init__()\n",
    "        self.upsample = ComplexUpSampling2D(size=(2, 2), interpolation='bilinear')\n",
    "        self.out_channels = out_channels\n",
    "        self.conv1 = complex_Conv2D(out_channels, kernel_size=3, padding='same')\n",
    "        self.conv2 = complex_Conv2D(out_channels, kernel_size=3, padding='same')\n",
    "        \n",
    "        # self.dense_block = DensBlock(out_channels)\n",
    "\n",
    "    def call(self, real_input1, imag_input1, real_input2, imag_input2):\n",
    "        # Upsample\n",
    "        up_real, up_imag = self.upsample(real_input2, imag_input2)\n",
    "\n",
    "        # Concatenate with skip connection\n",
    "        concat_real, concat_imag = concatenate_with(real_input1, imag_input1, up_real, up_imag)\n",
    "\n",
    "        # Proper flow through both conv layers\n",
    "        out_real, out_imag = self.conv1(concat_real, concat_imag)\n",
    "        out_real, out_imag = CReLU(out_real, out_imag)\n",
    "\n",
    "        out_real, out_imag = self.conv2(out_real, out_imag)\n",
    "        out_real, out_imag = CReLU(out_real, out_imag)\n",
    "\n",
    "        return out_real, out_imag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94c1a2f3-9338-49b8-ae3e-34d6123eacb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "class SF_UNet_TF(Model):\n",
    "    def __init__(self, input_shape=(320, 320, 2), num_classes=2):\n",
    "        super().__init__()\n",
    "        self.encoder = ComplexEncoderTF(input_shape=input_shape)\n",
    "\n",
    "\n",
    "        self.skip4 = SkipConnection(ratio=10)\n",
    "        self.skip3 = SkipConnection(ratio=10)\n",
    "        self.skip2 = SkipConnection(ratio=10)\n",
    "        self.skip1 = SkipConnection(ratio=10)\n",
    "\n",
    "        self.unet_up4 = ComplexUnetUpTF(512)\n",
    "        self.unet_up3 = ComplexUnetUpTF(256)\n",
    "        self.unet_up2 = ComplexUnetUpTF(128)\n",
    "        self.unet_up1 = ComplexUnetUpTF(64)\n",
    "\n",
    "        self.final_conv = complex_Conv2D(filters=num_classes, kernel_size=1, activation='linear')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # real = tf.expand_dims(inputs[..., 0], axis=-1)\n",
    "        # imag = tf.expand_dims(inputs[..., 1], axis=-1)\n",
    "\n",
    "        feat1, feat2, feat3, feat4, feat5 = self.encoder(inputs)\n",
    "        # print(\"feat4 real shape:\", feat4[0].shape)\n",
    "        # print(\"feat4 imag shape:\", feat4[1].shape)\n",
    "        # print(\"feat5 real shape:\", feat5[0].shape)\n",
    "        # print(\"feat5 imag shape:\", feat5[1].shape)\n",
    "\n",
    "        # Skip + up stage 4\n",
    "        # tf.print(\"feat4 real min:\", tf.reduce_min(feat4[0]), \"max:\", tf.reduce_max(feat4[0]), \"mean:\", tf.reduce_mean(feat4[0]))\n",
    "        # tf.print(\"feat4 imag min:\", tf.reduce_min(feat4[1]), \"max:\", tf.reduce_max(feat4[1]), \"mean:\", tf.reduce_mean(feat4[1]))\n",
    "        # tf.debugging.check_numerics(feat4[0], \"üö® feat4[0] has NaNs/Infs\")\n",
    "        # tf.debugging.check_numerics(feat4[1], \"üö® feat4[1] has NaNs/Infs\")\n",
    "\n",
    "        skip4_out_r, skip4_out_i = self.skip4(feat4[0],feat4[1])\n",
    "        # tf.print(\"Skip4 output shape (real):\", tf.shape(skip4_out_r))\n",
    "        # tf.print(\"Skip4 output shape (imag):\", tf.shape(skip4_out_i))\n",
    "\n",
    "        decoder4 = self.unet_up4(skip4_out_r, skip4_out_i, feat5[0], feat5[1])\n",
    "        # tf.print(\"UnetUp4 output shape (real):\", tf.shape(decoder4[0]))\n",
    "        # tf.print(\"UnetUp4 output shape (imag):\", tf.shape(decoder4[1]))\n",
    "\n",
    "        # Skip + up stage 3\n",
    "        skip3_out_r, skip3_out_i = self.skip3(feat3[0],feat3[1])\n",
    "        # tf.print(\"Skip3 output shape (real):\", tf.shape(skip3_out_r))\n",
    "        # tf.print(\"Skip3 output shape (imag):\", tf.shape(skip3_out_i))\n",
    "\n",
    "        decoder3 = self.unet_up3(skip3_out_r, skip3_out_i, decoder4[0], decoder4[1])\n",
    "        # tf.print(\"UnetUp3 output shape (real):\", tf.shape(decoder3[0]))\n",
    "        # tf.print(\"UnetUp3 output shape (imag):\", tf.shape(decoder3[1]))\n",
    "\n",
    "        # Skip + up stage 2\n",
    "        skip2_out_r, skip2_out_i = self.skip2(feat2[0],feat2[1])\n",
    "        # tf.print(\"Skip2 output shape (real):\", tf.shape(skip2_out_r))\n",
    "        # tf.print(\"Skip2 output shape (imag):\", tf.shape(skip2_out_i))\n",
    "\n",
    "        decoder2 = self.unet_up2(skip2_out_r, skip2_out_i, decoder3[0], decoder3[1])\n",
    "        # tf.print(\"UnetUp2 output shape (real):\", tf.shape(decoder2[0]))\n",
    "        # tf.print(\"UnetUp2 output shape (imag):\", tf.shape(decoder2[1]))\n",
    "\n",
    "        # Skip + up stage 1\n",
    "        skip1_out_r, skip1_out_i = self.skip1(feat1[0],feat1[1])\n",
    "        # tf.print(\"Skip1 output shape (real):\", tf.shape(skip1_out_r))\n",
    "        # tf.print(\"Skip1 output shape (imag):\", tf.shape(skip1_out_i))\n",
    "\n",
    "        decoder1 = self.unet_up1(skip1_out_r, skip1_out_i, decoder2[0], decoder2[1])\n",
    "        # tf.print(\"UnetUp1 output shape (real):\", tf.shape(decoder1[0]))\n",
    "        # tf.print(\"UnetUp1 output shape (imag):\", tf.shape(decoder1[1]))\n",
    "\n",
    "        # Final 1√ó1 conv\n",
    "        real_out, imag_out = self.final_conv(decoder1[0], decoder1[1])\n",
    "        # tf.print(\"Final conv output shape (real):\", tf.shape(real_out))\n",
    "        # tf.print(\"Final conv output shape (imag):\", tf.shape(imag_out))\n",
    "\n",
    "        # Concatenate back to a 2-channel or 4-channel tensor\n",
    "        output = concatenate([real_out, imag_out], axis=-1)\n",
    "        # tf.print(\"Final concatenated output shape:\", tf.shape(output))\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6ca92fb-6342-4a8a-83fd-c4a12deb7def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.layers import Input, Lambda\n",
    "# from tensorflow.keras import Model\n",
    "\n",
    "# def build_dual_output_model(nf=32, kshape=(3, 3), H=320, W=320, channels=2, cascades=5):\n",
    "#     input_img = Input(shape=(H, W, channels), name='input_img')\n",
    "\n",
    "#     # Step 1: First image-domain U-Net\n",
    "#     unet1 = SF_UNet_TF()\n",
    "#     img1_out = unet1(input_img)\n",
    "\n",
    "\n",
    "#     # Label output\n",
    "#     final_out_image = tf.keras.layers.Lambda(lambda x: x, name='target_img')(img1_out)\n",
    "\n",
    "#     # Only final image output (no k-space)\n",
    "#     model = Model(\n",
    "#         inputs=input_img,\n",
    "#         outputs={'target_img': final_out_image}\n",
    "#     )\n",
    "\n",
    "#     return model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "def build_dual_output_model(kshape=(3, 3), H=320, W=320, channels=2):\n",
    "    input_img = Input(shape=(H, W, channels))\n",
    "\n",
    "    # Image-domain SF_UNet_TF only\n",
    "    unet = SF_UNet_TF()\n",
    "    output_img = unet(input_img)\n",
    "\n",
    "    # Assign a name to the output for use in loss dict\n",
    "    output_img = Lambda(lambda x: x)(output_img)\n",
    "\n",
    "    model = Model(\n",
    "        inputs=input_img,\n",
    "        outputs=output_img\n",
    "    )\n",
    "\n",
    "    return model\n",
    "# model = build_dual_output_model(H=320, W=320)\n",
    "# model.summary()\n",
    "\n",
    "# # Create a dummy batch: batch=2, H=64, W=64, 2 channels (real+imag)\n",
    "# dummy = np.random.randn(2, 320, 320, 2).astype(np.float32)\n",
    "\n",
    "# # Forward-pass: this will trigger our tf.print in SkipConnection.call\n",
    "# _ = model(dummy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f3e5efe-4d91-41b7-9a0e-b72c5ae24378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.losses import MeanSquaredError\n",
    "\n",
    "# # Build the model\n",
    "# model = build_dual_output_model(H=320, W=320, channels=2)\n",
    "\n",
    "# # Create dummy input and target tensors\n",
    "# x = tf.random.normal([1, 320, 320, 2])  # input: real + imag\n",
    "# y_true = tf.random.normal([1, 320, 320, 2])  # target: real + imag\n",
    "\n",
    "# # Use Mean Squared Error as loss\n",
    "# loss_fn = MeanSquaredError()\n",
    "\n",
    "# # Track gradients\n",
    "# with tf.GradientTape() as tape:\n",
    "#     y_pred = model(x)\n",
    "#     loss = loss_fn(y_true, y_pred)\n",
    "\n",
    "# # Get trainable variables and compute gradients\n",
    "# trainable_vars = model.trainable_variables\n",
    "# grads = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "# # Check which gradients are None\n",
    "# none_grads = [var.name for var, g in zip(trainable_vars, grads) if g is None]\n",
    "\n",
    "# # Display result\n",
    "# if none_grads:\n",
    "#     print(\"‚ö†Ô∏è Gradients are NOT flowing for these variables:\")\n",
    "#     for name in none_grads:\n",
    "#         print(f\" - {name}\")\n",
    "# else:\n",
    "#     print(\"‚úÖ All gradients are flowing correctly!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0757837-86d4-4471-93ef-6cc48d12bd49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfdc848-9483-4973-a9eb-9b1c91903626",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1a7ba6-3d98-4512-8c9a-4beba541540d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
