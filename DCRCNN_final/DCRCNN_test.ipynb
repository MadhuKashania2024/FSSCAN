{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bf714bf-43f1-4aac-8e9f-d3b6ae4e8804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45b95867-891b-485d-bbff-727bcefaba07",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Research\\\\aman fastmri\\\\Data\\\\mask_4x_320_random.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mResearch\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43maman fastmri\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mData\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mmask_4x_320_random.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Shape: (1, 320, 320)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mog shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, mask\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\WNet\\lib\\site-packages\\numpy\\lib\\npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Research\\\\aman fastmri\\\\Data\\\\mask_4x_320_random.npy'"
     ]
    }
   ],
   "source": [
    "mask = np.load(r\"C:\\Users\\Research\\aman fastmri\\Data\\mask_4x_320_random.npy\")  # Shape: (1, 320, 320)\n",
    "print(\"og shape:\", mask.shape)\n",
    "\n",
    "# # Use np.tile to reshape it to (1, 320, 320, 1)\n",
    "# # var_sampling_mask = np.tile(var_sampling_mask[..., np.newaxis], (1, 1, 1, 1))  # Final shape: (1, 320, 320, 1)\n",
    "# mask = np.tile(mask, (1, 320, 1, 2))  # tile height=320 times\n",
    "\n",
    "# # Confirm final shape\n",
    "# print(\"New shape:\", mask.shape) \n",
    "# mask_for_plot = np.squeeze(mask[...,0])  # Shape: (320, 320)\n",
    "\n",
    "# # Plot\n",
    "# plt.figure(figsize=(5, 5))\n",
    "# plt.imshow(mask_for_plot, cmap='gray')\n",
    "# plt.title(\"Tiled Sampling Mask (320x320)\")\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1621cbc7-8db4-40c3-a553-61512092397b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def to_complex(x):\n",
    "    return x[..., 0] + 1j * x[..., 1]\n",
    "\n",
    "class MRISliceGeneratorMag(tf.keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    Data generator for magnitude-only MRI reconstruction.\n",
    "\n",
    "    Input  : undersampled magnitude image  (B, H, W, 1)\n",
    "    Target : fully-sampled magnitude image (B, H, W, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_list, batch_size=4, shuffle=True):\n",
    "        self.file_list = file_list\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.slice_index_map = []\n",
    "        self._build_index()\n",
    "\n",
    "    def _build_index(self):\n",
    "        for file_idx, file_path in enumerate(self.file_list):\n",
    "            with h5py.File(file_path, 'r') as f:\n",
    "                num_slices = f['image_under'].shape[0]\n",
    "                for slice_idx in range(num_slices):\n",
    "                    self.slice_index_map.append((file_idx, slice_idx))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.slice_index_map) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_map = self.slice_index_map[\n",
    "            index * self.batch_size : (index + 1) * self.batch_size\n",
    "        ]\n",
    "\n",
    "        input_mag_batch = []\n",
    "        target_mag_batch = []\n",
    "\n",
    "        for file_idx, slice_idx in batch_map:\n",
    "            with h5py.File(self.file_list[file_idx], 'r') as f:\n",
    "                img_under = f['image_under'][slice_idx]  # (H, W, 2)\n",
    "                img_full  = f['image_full'][slice_idx]   # (H, W, 2)\n",
    "\n",
    "                # Convert to complex\n",
    "                img_under_c = to_complex(img_under)\n",
    "                img_full_c  = to_complex(img_full)\n",
    "\n",
    "                # Magnitude\n",
    "                img_under_mag = np.abs(img_under_c)\n",
    "                img_full_mag  = np.abs(img_full_c)\n",
    "\n",
    "                input_mag_batch.append(img_under_mag)\n",
    "                target_mag_batch.append(img_full_mag)\n",
    "\n",
    "        # Stack and add channel dimension\n",
    "        x_batch = np.stack(input_mag_batch, axis=0)[..., np.newaxis]  # (B, H, W, 1)\n",
    "        y_batch = np.stack(target_mag_batch, axis=0)[..., np.newaxis] # (B, H, W, 1)\n",
    "\n",
    "        return x_batch, y_batch\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.slice_index_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06b2909b-87d9-40de-abd3-f8b19ebb8755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_folder = r\"E:\\fastmri_singlecoil_FSSCAN\\val_norm\"\n",
    "import h5py\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "kspace_files_list_val = sorted(glob.glob(os.path.join(val_folder, \"*.h5\")))\n",
    "\n",
    "half_val = len(kspace_files_list_val) \n",
    "half_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32d285ec-3f9e-4064-9555-ad98a7a56ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7135\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "kspace_files_list_val = sorted(glob.glob(os.path.join(val_folder, \"*.h5\")))\n",
    "\n",
    "\n",
    "half_val = len(kspace_files_list_val) \n",
    "\n",
    "kspace_files_list_val = kspace_files_list_val[:half_val]\n",
    "\n",
    "\n",
    "val_gen = MRISliceGeneratorMag(kspace_files_list_val, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "print(len(val_gen))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb595645-da9c-4b29-97f5-36b965a6c814",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./DCRCNN.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1179648a-3e04-4575-ab4c-d6863eb19283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# ============================================================\n",
    "# Directory Setup\n",
    "# ============================================================\n",
    "save_dir = \"./SavedModels_DCRCNN_full_2\"\n",
    "H, W = 320, 320\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6964654c-d58d-49cd-ae24-dc505fbba028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Total parameters:       1.202 M\n",
      "Trainable parameters:   1.202 M\n",
      "Non-trainable params:   0.000 M\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "model = build_dcr_cnn(\n",
    "    input_shape=(320, 320, 2),\n",
    "    num_dcr_blocks=10,   # or 3 / 8\n",
    "    num_features=64,\n",
    "    growth_rate=32\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "def count_parameters_millions(model):\n",
    "    trainable = np.sum([np.prod(v.shape) for v in model.trainable_variables])\n",
    "    non_trainable = np.sum([np.prod(v.shape) for v in model.non_trainable_variables])\n",
    "    total = trainable + non_trainable\n",
    "    return (\n",
    "        total / 1e6,\n",
    "        trainable / 1e6,\n",
    "        non_trainable / 1e6\n",
    "    )\n",
    "\n",
    "total_M, trainable_M, non_trainable_M = count_parameters_millions(model)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(f\"Total parameters:       {total_M:.3f} M\")\n",
    "print(f\"Trainable parameters:   {trainable_M:.3f} M\")\n",
    "print(f\"Non-trainable params:   {non_trainable_M:.3f} M\")\n",
    "print(\"=\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "508224d8-35bc-496c-b671-418ec99c2bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
    "\n",
    "def compute_flops(model, input_shape):\n",
    "    \"\"\"\n",
    "    input_shape: tuple, e.g. (1, H, W, 2)\n",
    "    returns FLOPs (float operations) for one forward pass\n",
    "    \"\"\"\n",
    "\n",
    "    @tf.function\n",
    "    def forward(x):\n",
    "        return model(x)\n",
    "\n",
    "    concrete_func = forward.get_concrete_function(\n",
    "        tf.TensorSpec(input_shape, tf.float32)\n",
    "    )\n",
    "\n",
    "    frozen_func = convert_variables_to_constants_v2(concrete_func)\n",
    "    graph_def = frozen_func.graph.as_graph_def()\n",
    "\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.graph_util.import_graph_def(graph_def, name=\"\")\n",
    "\n",
    "        run_meta = tf.compat.v1.RunMetadata()\n",
    "        opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
    "\n",
    "        flops = tf.compat.v1.profiler.profile(\n",
    "            graph=graph,\n",
    "            run_meta=run_meta,\n",
    "            cmd=\"op\",\n",
    "            options=opts\n",
    "        )\n",
    "\n",
    "    return flops.total_float_ops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ec7317d-f315-4c1a-8029-f0dd3b5916ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Research\\anaconda3\\envs\\WNet\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:5250: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n",
      "FLOPs (single forward pass): 245.98 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "# Example: infer H, W from your data or define explicitly\n",
    "H, W = 320,320\n",
    "\n",
    "flops = compute_flops(model, input_shape=(1, H, W, 2))\n",
    "\n",
    "print(f\"FLOPs (single forward pass): {flops / 1e9:.2f} GFLOPs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43ddae09-31d0-4e84-bb18-f2a5326c42b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded latest checkpoint\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ============================================================\n",
    "# Optimizer & Compile\n",
    "# ============================================================\n",
    "optimizer = Adam(learning_rate=LEARNING_RATE)\n",
    "model.compile(optimizer=optimizer, loss=\"mse\")\n",
    "\n",
    "# ============================================================\n",
    "# Load Initial Weights (Optional Resume)\n",
    "# ============================================================\n",
    "if tf.train.latest_checkpoint(save_dir):\n",
    "    model.load_weights(tf.train.latest_checkpoint(save_dir))\n",
    "    print(\"✅ Loaded latest checkpoint\")\n",
    "else:\n",
    "    print(\"ℹ️ No checkpoint found. Training from scratch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "baa899f6-7750-4ddf-9b3f-2efa9c32f11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing volumes: 100%|████████████████████████████████████████████████████████████| 199/199 [07:56<00:00,  2.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Excel file saved to: evaluation_metrics_DCRCNN.xlsx\n",
      "==================================================\n",
      "PSNR (Mag, volume): 32.6638 ± 2.8763 dB\n",
      "NMSE (Mag, volume): 0.027041 ± 0.017342\n",
      "SSIM (Mag, slice):  0.7634 ± 0.0853\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
    "\n",
    "# --------------------------------------------------\n",
    "# LOAD MODEL WEIGHTS\n",
    "# --------------------------------------------------\n",
    "#model.load_weights(\"./weight_sfu_fastmri_complex_perploss_fsa.h5\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# HELPERS\n",
    "# --------------------------------------------------\n",
    "def to_complex(x):\n",
    "    return x[..., 0] + 1j * x[..., 1]\n",
    "\n",
    "def nmse(gt, pred):\n",
    "    return np.linalg.norm(gt - pred) ** 2 / (np.linalg.norm(gt) ** 2)\n",
    "\n",
    "def compute_ssim(gt, pred, max_val):\n",
    "    return structural_similarity(\n",
    "        gt,\n",
    "        pred,\n",
    "        data_range=max_val,\n",
    "        win_size=9,\n",
    "        gaussian_weights=False,\n",
    "        use_sample_covariance=False,\n",
    "        K1=0.01,\n",
    "        K2=0.03,\n",
    "        full=False\n",
    "    )\n",
    "\n",
    "# --------------------------------------------------\n",
    "# STORAGE (FOR EXCEL)\n",
    "# --------------------------------------------------\n",
    "volume_metrics = []   # volume-wise PSNR, NMSE\n",
    "slice_metrics  = []   # slice-wise SSIM\n",
    "ssim_list = []\n",
    "psnr_list = []\n",
    "nmse_list = []\n",
    "\n",
    "# --------------------------------------------------\n",
    "# PROCESSING\n",
    "# --------------------------------------------------\n",
    "for file in tqdm(kspace_files_list_val, desc=\"Processing volumes\"):\n",
    "    volume_name = os.path.basename(file)\n",
    "\n",
    "    with h5py.File(file, 'r') as f:\n",
    "        image_full  = f[\"image_full\"][:]    # (S, H, W, 2)\n",
    "        image_under = f[\"image_under\"][:]   # (S, H, W, 2)\n",
    "        max_val     = float(f[\"max_val_full_image\"][0])\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Model inference\n",
    "    # --------------------------------------------------\n",
    "    pred = model.predict(image_under, verbose=0)  # (S, H, W, 2)\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Rescale\n",
    "    # --------------------------------------------------\n",
    "    image_full = image_full * max_val\n",
    "    pred       = pred * max_val\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Magnitude images\n",
    "    # --------------------------------------------------\n",
    "    gt_mag   = np.abs(to_complex(image_full))\n",
    "    pred_mag = np.abs(to_complex(pred))\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # pred *= max_val  # Scale predicted output to original intensity range\n",
    "    psnr_val = peak_signal_noise_ratio(gt_mag, pred_mag, data_range=max_val)\n",
    "\n",
    "    # Volume-wise PSNR and NMSE\n",
    "    #psnr_val = peak_signal_noise_ratio(gt_mag,pred_mag, data_range=max_val)\n",
    "    nmse_val = nmse(gt_mag.flatten(), pred_mag.flatten())\n",
    "\n",
    "    psnr_list.append(psnr_val)\n",
    "    nmse_list.append(nmse_val)\n",
    "\n",
    "    volume_metrics.append({\n",
    "        \"volume_name\": volume_name,\n",
    "        \"num_slices\": gt_mag.shape[0],\n",
    "        \"PSNR_dB\": psnr_val,\n",
    "        \"NMSE\": nmse_val\n",
    "    })\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Slice-wise SSIM (2D per slice)\n",
    "    # --------------------------------------------------\n",
    "    for i in range(gt_mag.shape[0]):\n",
    "        ssim_val = compute_ssim(\n",
    "            gt_mag[i],        # (H, W)\n",
    "            pred_mag[i],      # (H, W)\n",
    "            max_val\n",
    "        )\n",
    "\n",
    "        slice_metrics.append({\n",
    "            \"volume_name\": volume_name,\n",
    "            \"slice_index\": i,\n",
    "            \"SSIM\": ssim_val\n",
    "        })\n",
    "\n",
    "# --------------------------------------------------\n",
    "# SAVE TO EXCEL\n",
    "# --------------------------------------------------\n",
    "df_volume = pd.DataFrame(volume_metrics)\n",
    "df_slice  = pd.DataFrame(slice_metrics)\n",
    "\n",
    "output_excel = \"evaluation_metrics_DCRCNN.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(output_excel, engine=\"openpyxl\") as writer:\n",
    "    df_volume.to_excel(writer, sheet_name=\"Volume_Metrics\", index=False)\n",
    "    df_slice.to_excel(writer, sheet_name=\"Slice_SSIM\", index=False)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# REPORT\n",
    "# --------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(f\"Excel file saved to: {output_excel}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"PSNR (Mag, volume): {df_volume['PSNR_dB'].mean():.4f} ± {df_volume['PSNR_dB'].std():.4f} dB\")\n",
    "print(f\"NMSE (Mag, volume): {df_volume['NMSE'].mean():.6f} ± {df_volume['NMSE'].std():.6f}\")\n",
    "print(f\"SSIM (Mag, slice):  {df_slice['SSIM'].mean():.4f} ± {df_slice['SSIM'].std():.4f}\")\n",
    "print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12b9d327-02d4-42e7-bc63-516849a00815",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing volumes: 100%|████████████████████████████████████████████████████████████| 199/199 [06:36<00:00,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "PSNR : 32.6637 ± 2.8691 dB\n",
      "NMSE (Mag, volume): 0.027042 ± 0.017300\n",
      "SSIM (Mag, slice):  0.7634 ± 0.0853\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
    "\n",
    "# # Path to validation folder\n",
    "# # val_folder = \"F:/denoised_preprocessed_h5_val\"\n",
    "\n",
    "# val_folder = r\"E:\\fastmri\\val_norm\"\n",
    "# # val_folder = r\"D:\\val_norm\"\n",
    "\n",
    "# # val_folder = r\"G:\\val_norm\\val_norm\"\n",
    "# # files = sorted([os.path.join(val_folder, f) for f in os.listdir(val_folder) if f.endswith(\".h5\")])\n",
    "# kspace_files_list_val = sorted(glob.glob(os.path.join(val_folder, \"*.h5\")))\n",
    "file_paths = kspace_files_list_val\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# HELPERS\n",
    "# ----------------------\n",
    "def to_complex(x):\n",
    "    return x[..., 0] + 1j * x[..., 1]\n",
    "\n",
    "def nmse(gt, pred):\n",
    "    return np.linalg.norm(gt - pred) ** 2 / (np.linalg.norm(gt) ** 2 + 1e-10)\n",
    "\n",
    "def compute_ssim(gt, pred, max_val):\n",
    "    return structural_similarity(\n",
    "        gt, pred,\n",
    "        data_range=max_val,\n",
    "        win_size=9,\n",
    "        gaussian_weights=False,\n",
    "        use_sample_covariance=False,\n",
    "        K1=0.01,\n",
    "        K2=0.03\n",
    "    )\n",
    "\n",
    "# ----------------------\n",
    "# STORAGE\n",
    "# ----------------------\n",
    "ssim_list = []\n",
    "psnr_list = []\n",
    "nmse_list = []\n",
    "\n",
    "# ----------------------\n",
    "# PROCESSING\n",
    "# ----------------------\n",
    "for file in tqdm(kspace_files_list_val, desc=\"Processing volumes\"):\n",
    "    with h5py.File(file, 'r') as f:\n",
    "        image_under = f[\"image_under\"][:]   # (S, H, W, 2)\n",
    "        image_full   = f[\"image_full\"][:]     # (S, H, W, 2)\n",
    "        max_val = float(f[\"max_val_full_image\"][0])\n",
    "\n",
    "    num_slices = image_under.shape[0]\n",
    "        # Convert to complex\n",
    "    # img_under_c = to_complex(image_under)\n",
    "    # img_full_c  = to_complex(image_full)\n",
    "\n",
    "    # # Magnitude\n",
    "    # img_under_mag = np.abs(img_under_c)\n",
    "    # img_full_mag  = np.abs(img_full_c)\n",
    "    # img_full_mag = np.expand_dims(img_full_mag, axis=-1)\n",
    "\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # TILE MASK FOR THIS VOLUME\n",
    "    # --------------------------------------------------\n",
    "    #mask_batch = np.tile(mask, (num_slices, 1, 1, 1))\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # MODEL INFERENCE\n",
    "    # --------------------------------------------------\n",
    "    pred = model.predict(\n",
    "        image_under,\n",
    "        batch_size=1,\n",
    "        verbose=0\n",
    "    )\n",
    "    #print(\"pred\",pred.shape)\n",
    "    #print(\"img_full_mag\",img_full_mag.shape)\n",
    "    \n",
    "\n",
    "    #max_val=img_full_mag.max()\n",
    "    image_full *= max_val\n",
    "    \n",
    "    pred *= max_val  # Scale predicted output to original intensity range\n",
    "    \n",
    "\n",
    "    # # Convert to complex and get magnitude\n",
    "    gt_mag = np.abs(to_complex(image_full))\n",
    "    pred_mag = np.abs(to_complex(pred))\n",
    "\n",
    "    # Volume-wise PSNR and NMSE\n",
    "    #psnr_val = peak_signal_noise_ratio(gt_mag, pred_mag, data_range=max_val)\n",
    "    nmse_val = nmse(gt_mag.flatten(), pred_mag.flatten())\n",
    "    psnr_val = peak_signal_noise_ratio(gt_mag, pred_mag, data_range=max_val)\n",
    "\n",
    "    psnr_list.append(psnr_val)\n",
    "    nmse_list.append(nmse_val)\n",
    "\n",
    "    # Slice-wise SSIM\n",
    "    for i in range(gt_mag.shape[0]):\n",
    "        ssim_val = compute_ssim(gt_mag[i], pred_mag[i], max_val)\n",
    "        ssim_list.append(ssim_val)\n",
    "# ----------------------\n",
    "# REPORT\n",
    "# ----------------------\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(f\"PSNR : {np.mean(psnr_list):.4f} ± {np.std(psnr_list):.4f} dB\")\n",
    "print(f\"NMSE (Mag, volume): {np.mean(nmse_list):.6f} ± {np.std(nmse_list):.6f}\")\n",
    "print(f\"SSIM (Mag, slice):  {np.mean(ssim_list):.4f} ± {np.std(ssim_list):.4f}\")\n",
    "\n",
    "print(\"=\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381c7eca-2506-4c91-b656-787e2eeeeea5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
