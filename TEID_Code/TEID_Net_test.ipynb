{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dcf6765-8bbe-4be5-b19d-8f195e6578f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95fe77b6-2559-4245-adf1-21f776a38441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "og shape: (1, 1, 320, 1)\n"
     ]
    }
   ],
   "source": [
    "mask = np.load(r\"C:\\Users\\DU\\aman_fastmri\\Data\\mask_4x_320_random.npy\")  # Shape: (1, 320, 320)\n",
    "print(\"og shape:\", mask.shape)\n",
    "\n",
    "# # Use np.tile to reshape it to (1, 320, 320, 1)\n",
    "# # var_sampling_mask = np.tile(var_sampling_mask[..., np.newaxis], (1, 1, 1, 1))  # Final shape: (1, 320, 320, 1)\n",
    "# mask = np.tile(mask, (1, 320, 1, 2))  # tile height=320 times\n",
    "\n",
    "# # Confirm final shape\n",
    "# print(\"New shape:\", mask.shape) \n",
    "# mask_for_plot = np.squeeze(mask[...,0])  # Shape: (320, 320)\n",
    "\n",
    "# # Plot\n",
    "# plt.figure(figsize=(5, 5))\n",
    "# plt.imshow(mask_for_plot, cmap='gray')\n",
    "# plt.title(\"Tiled Sampling Mask (320x320)\")\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b4c89d8-9bdb-4135-ad2d-98a331716150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class MRISliceGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, file_list, batch_size=4, shuffle=True, mask=None):\n",
    "        self.file_list = file_list\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.mask = mask  # Shape: (1, 320, 320, 2)\n",
    "        self.slice_index_map = []\n",
    "        self._build_index()\n",
    "\n",
    "    def _build_index(self):\n",
    "        for file_idx, file_path in enumerate(self.file_list):\n",
    "            with h5py.File(file_path, 'r') as f:\n",
    "                num_slices = f['image_under'].shape[0]\n",
    "                for slice_idx in range(num_slices):\n",
    "                    self.slice_index_map.append((file_idx, slice_idx))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.slice_index_map) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_map = self.slice_index_map[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        input_img_batch = []\n",
    "        target_img_batch = []\n",
    "        input_kspace_batch = []\n",
    "\n",
    "        for file_idx, slice_idx in batch_map:\n",
    "            with h5py.File(self.file_list[file_idx], 'r') as f:\n",
    "                #input_img = f['image_under'][slice_idx]       # shape: [H, W, 2]\n",
    "                target_img = f['image_full'][slice_idx]       # shape: [H, W, 2]\n",
    "                input_kspace = f['kspace_under'][slice_idx]   # shape: [H, W, 2]\n",
    "\n",
    "                #input_img_batch.append(input_img)\n",
    "                target_img_batch.append(target_img)\n",
    "                input_kspace_batch.append(input_kspace)\n",
    "\n",
    "        #x_img = np.stack(input_img_batch, axis=0)\n",
    "        x_kspace = np.stack(input_kspace_batch, axis=0)\n",
    "        y_batch = np.stack(target_img_batch, axis=0)\n",
    "\n",
    "        if self.mask is not None:\n",
    "            actual_batch_size = len(x_kspace)\n",
    "\n",
    "            # mask must be (1, 1, W, 1)\n",
    "            if self.mask.shape != (1, 1, 320, 1):\n",
    "                raise ValueError(\"Mask must have shape (1, 1, W, 1)\")\n",
    "        \n",
    "            # Tile ONLY along batch dimension\n",
    "            mask_batch = np.tile(self.mask, (actual_batch_size, 1, 1, 1))\n",
    "        \n",
    "        return [x_kspace, mask_batch], y_batch\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.slice_index_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63fe1568-f86d-41cd-9550-59b3171e6a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_folder = r\"D:\\fastmri_singlecoil_FSSCAN\\val_norm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb5b1076-bc18-42c0-8bc9-04e9b3a85c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1784\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "kspace_files_list_val = sorted(glob.glob(os.path.join(val_folder, \"*.h5\")))\n",
    "\n",
    "\n",
    "half_val = len(kspace_files_list_val) \n",
    "\n",
    "kspace_files_list_val = kspace_files_list_val[:half_val]\n",
    "\n",
    "\n",
    "val_gen = MRISliceGenerator(kspace_files_list_val, batch_size=4, shuffle=False,mask=mask)\n",
    "# train_gen = MRISliceGenerator(kspace_files_list_train,batch_size=16, shuffle=True)\n",
    "# val_gen = MRISliceGenerator(kspace_files_list_val, batch_size=4, shuffle=False)\n",
    "\n",
    "\n",
    "print(len(val_gen))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "152e4da4-f29a-4320-b0df-ee5e889ce1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CascadedTEIDNet_Model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " kspace_undersampled (InputLaye  [(None, 320, 320, 2  0          []                               \n",
      " r)                             )]                                                                \n",
      "                                                                                                  \n",
      " sampling_mask (InputLayer)     [(None, 1, 320, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " cascaded_teid_net (CascadedTEI  (None, 320, 320, 2)  2521475    ['kspace_undersampled[0][0]',    \n",
      " DNet)                                                            'sampling_mask[0][0]']          \n",
      "                                                                                                  \n",
      " reconstructed_image (Lambda)   (None, 320, 320, 2)  0           ['cascaded_teid_net[0][0]']      \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,521,475\n",
      "Trainable params: 2,521,475\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "%run ./TEID_Net.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7edf3632-8fa3-4c83-972c-8d626ec3f4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Total parameters:       2.521 M\n",
      "Trainable parameters:   2.521 M\n",
      "Non-trainable params:   0.000 M\n",
      "========================================\n",
      "FLOPs (single forward pass): 274.85 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def count_parameters_millions(model):\n",
    "    trainable = np.sum([np.prod(v.shape) for v in model.trainable_variables])\n",
    "    non_trainable = np.sum([np.prod(v.shape) for v in model.non_trainable_variables])\n",
    "    total = trainable + non_trainable\n",
    "    return (\n",
    "        total / 1e6,\n",
    "        trainable / 1e6,\n",
    "        non_trainable / 1e6\n",
    "    )\n",
    "\n",
    "total_M, trainable_M, non_trainable_M = count_parameters_millions(model)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(f\"Total parameters:       {total_M:.3f} M\")\n",
    "print(f\"Trainable parameters:   {trainable_M:.3f} M\")\n",
    "print(f\"Non-trainable params:   {non_trainable_M:.3f} M\")\n",
    "print(\"=\" * 40)\n",
    "import tensorflow as tf\n",
    "def compute_flops_multi_input(model, input_shapes):\n",
    "    \"\"\"\n",
    "    input_shapes: list of tuples\n",
    "      e.g. [(1, H, W, 2), (1, 1, W, 1)]\n",
    "    \"\"\"\n",
    "\n",
    "    @tf.function\n",
    "    def forward(*inputs):\n",
    "        return model(inputs)\n",
    "\n",
    "    concrete_func = forward.get_concrete_function(\n",
    "        *[tf.TensorSpec(shape, tf.float32) for shape in input_shapes]\n",
    "    )\n",
    "\n",
    "    frozen_func = convert_variables_to_constants_v2(concrete_func)\n",
    "    graph_def = frozen_func.graph.as_graph_def()\n",
    "\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.graph_util.import_graph_def(graph_def, name=\"\")\n",
    "\n",
    "        run_meta = tf.compat.v1.RunMetadata()\n",
    "        opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
    "\n",
    "        flops = tf.compat.v1.profiler.profile(\n",
    "            graph=graph,\n",
    "            run_meta=run_meta,\n",
    "            cmd=\"op\",\n",
    "            options=opts\n",
    "        )\n",
    "\n",
    "    return flops.total_float_ops\n",
    "H, W = 320, 320\n",
    "\n",
    "input_shapes = [\n",
    "    (1, H, W, 2),   # kspace_undersampled\n",
    "    (1, 1, W, 1)    # sampling_mask\n",
    "]\n",
    "\n",
    "flops = compute_flops_multi_input(model, input_shapes)\n",
    "\n",
    "print(f\"FLOPs (single forward pass): {flops / 1e9:.2f} GFLOPs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28742f21-376b-453d-9beb-cb84b5cfe1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded latest checkpoint\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# ============================================================\n",
    "# Directory Setup\n",
    "# ============================================================\n",
    "save_dir = \"./SavedModels_TEID_Net_lr_full\"\n",
    "H, W = 320, 320\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 1e-4\n",
    "model = build_cascaded_teid_model(\n",
    "    H=H,\n",
    "    W=W,\n",
    "    num_cascades=5,\n",
    "    channels=64\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# Optimizer & Compile\n",
    "# ============================================================\n",
    "optimizer = Adam(learning_rate=LEARNING_RATE)\n",
    "model.compile(optimizer=optimizer, loss=\"mae\")\n",
    "\n",
    "# ============================================================\n",
    "# Load Initial Weights (Optional Resume)\n",
    "# ============================================================\n",
    "if tf.train.latest_checkpoint(save_dir):\n",
    "    model.load_weights(tf.train.latest_checkpoint(save_dir))\n",
    "    print(\"‚úÖ Loaded latest checkpoint\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No checkpoint found. Training from scratch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8d4a6de-4402-4e7f-bcf7-f07df20bcebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\DU\\anaconda3\\envs\\WNet\\lib\\site-packages\\tensorflow\\python\\profiler\\internal\\flops_registry.py:138: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Timing on /CPU:0:   1%|‚ñå                                                           | 2/199 [13:28<22:07:43, 404.38s/it]\n",
      "Timing on /GPU:0:   1%|‚ñå                                                             | 2/199 [02:20<3:50:52, 70.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MODEL EFFICIENCY REPORT (BATCH SIZE = 1)\n",
      "======================================================================\n",
      "Parameters: 2.52 M\n",
      "FLOPs:      275.22 GFLOPs (per slice)\n",
      "\n",
      "--- CPU Inference ---\n",
      "Latency:    8.09 s / slice\n",
      "Throughput:0.124 slices/sec\n",
      "Memory:     12.40 MB\n",
      "\n",
      "--- GPU Inference ---\n",
      "GPU:        NVIDIA RTX A5000\n",
      "Latency:    1.41 s / slice\n",
      "Throughput:0.711 slices/sec\n",
      "Peak VRAM:  226.76 MB\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import glob\n",
    "import psutil\n",
    "import numpy as np\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from keras_flops import get_flops\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "VAL_FOLDER = r\"D:\\fastmri_singlecoil_FSSCAN\\val_norm\"\n",
    "WARMUP_SLICES = 10\n",
    "NUM_TIMING_SLICES = 100   # increase if needed\n",
    "\n",
    "# ============================================================\n",
    "# FILE LIST\n",
    "# ============================================================\n",
    "file_paths = sorted(glob.glob(os.path.join(VAL_FOLDER, \"*.h5\")))\n",
    "\n",
    "# ============================================================\n",
    "# MODEL & MASK MUST BE LOADED\n",
    "# ============================================================\n",
    "# model = ...\n",
    "# mask  = ...   # shape (1, H, W, 1) or (1, H, W, 2)\n",
    "\n",
    "assert model is not None, \"Model is not loaded.\"\n",
    "assert mask is not None, \"Mask is not loaded.\"\n",
    "\n",
    "# ============================================================\n",
    "# PARAMETER COUNT\n",
    "# ============================================================\n",
    "num_params = model.count_params()\n",
    "\n",
    "# ============================================================\n",
    "# FLOPs (PER SLICE, BATCH SIZE = 1)\n",
    "# ============================================================\n",
    "flops = get_flops(model, batch_size=1)\n",
    "\n",
    "# ============================================================\n",
    "# MEMORY HELPERS\n",
    "# ============================================================\n",
    "process = psutil.Process(os.getpid())\n",
    "\n",
    "def cpu_memory_mb():\n",
    "    return process.memory_info().rss / (1024 ** 2)\n",
    "\n",
    "def gpu_memory_mb():\n",
    "    info = tf.config.experimental.get_memory_info(\"GPU:0\")\n",
    "    return info[\"peak\"] / (1024 ** 2)\n",
    "\n",
    "# ============================================================\n",
    "# LATENCY / THROUGHPUT MEASUREMENT\n",
    "# ============================================================\n",
    "def measure_latency(device):\n",
    "\n",
    "    latencies = []\n",
    "\n",
    "    with tf.device(device):\n",
    "\n",
    "        # -----------------------------\n",
    "        # WARM-UP\n",
    "        # -----------------------------\n",
    "        for file in file_paths[:1]:\n",
    "            with h5py.File(file, \"r\") as f:\n",
    "                kspace_under = f[\"kspace_under\"][:]\n",
    "\n",
    "            for s in range(min(WARMUP_SLICES, kspace_under.shape[0])):\n",
    "                slice_k = kspace_under[s:s+1]\n",
    "                slice_m = mask\n",
    "                _ = model([slice_k, slice_m], training=False)\n",
    "\n",
    "        # -----------------------------\n",
    "        # TIMED INFERENCE\n",
    "        # -----------------------------\n",
    "        count = 0\n",
    "        for file in tqdm(file_paths, desc=f\"Timing on {device}\"):\n",
    "\n",
    "            with h5py.File(file, \"r\") as f:\n",
    "                kspace_under = f[\"kspace_under\"][:]\n",
    "\n",
    "            for s in range(kspace_under.shape[0]):\n",
    "\n",
    "                if count >= NUM_TIMING_SLICES:\n",
    "                    break\n",
    "\n",
    "                slice_k = kspace_under[s:s+1]   # (1, H, W, 2)\n",
    "                slice_m = mask                  # (1, H, W, 1 or 2)\n",
    "\n",
    "                assert slice_k.shape[0] == 1\n",
    "\n",
    "                start = time.perf_counter()\n",
    "                _ = model([slice_k, slice_m], training=False)\n",
    "\n",
    "                # üîë GPU synchronization\n",
    "                if \"GPU\" in device:\n",
    "                    tf.config.experimental.get_memory_info(\"GPU:0\")\n",
    "\n",
    "                end = time.perf_counter()\n",
    "\n",
    "                latencies.append(end - start)\n",
    "                count += 1\n",
    "\n",
    "            if count >= NUM_TIMING_SLICES:\n",
    "                break\n",
    "\n",
    "    latencies = np.array(latencies)\n",
    "\n",
    "    mean_s = latencies.mean()\n",
    "    median_s = np.median(latencies)\n",
    "\n",
    "    return {\n",
    "        \"mean_s\": mean_s,\n",
    "        \"median_s\": median_s,\n",
    "        \"std_s\": latencies.std(),\n",
    "        \"slices_per_sec\": 1.0 / mean_s\n",
    "    }\n",
    "\n",
    "# ============================================================\n",
    "# CPU BENCHMARK\n",
    "# ============================================================\n",
    "cpu_mem_before = cpu_memory_mb()\n",
    "cpu_latency = measure_latency(\"/CPU:0\")\n",
    "cpu_mem_after = cpu_memory_mb()\n",
    "cpu_mem_peak = cpu_mem_after - cpu_mem_before\n",
    "\n",
    "# ============================================================\n",
    "# GPU BENCHMARK (IF AVAILABLE)\n",
    "# ============================================================\n",
    "gpu_latency = None\n",
    "gpu_mem_peak = None\n",
    "gpu_name = None\n",
    "\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    tf.config.experimental.reset_memory_stats(\"GPU:0\")\n",
    "    gpu_name = tf.config.experimental.get_device_details(gpus[0])[\"device_name\"]\n",
    "    gpu_latency = measure_latency(\"/GPU:0\")\n",
    "    gpu_mem_peak = gpu_memory_mb()\n",
    "\n",
    "# ============================================================\n",
    "# FINAL REPORT\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL EFFICIENCY REPORT (BATCH SIZE = 1)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"Parameters: {num_params / 1e6:.2f} M\")\n",
    "print(f\"FLOPs:      {flops / 1e9:.2f} GFLOPs (per slice)\")\n",
    "\n",
    "print(\"\\n--- CPU Inference ---\")\n",
    "print(f\"Latency:    {cpu_latency['mean_s']:.2f} s / slice\")\n",
    "print(f\"Throughput:{cpu_latency['slices_per_sec']:.3f} slices/sec\")\n",
    "print(f\"Memory:     {cpu_mem_peak:.2f} MB\")\n",
    "\n",
    "if gpu_latency:\n",
    "    print(\"\\n--- GPU Inference ---\")\n",
    "    print(f\"GPU:        {gpu_name}\")\n",
    "    print(f\"Latency:    {gpu_latency['mean_s']:.2f} s / slice\")\n",
    "    print(f\"Throughput:{gpu_latency['slices_per_sec']:.3f} slices/sec\")\n",
    "    print(f\"Peak VRAM:  {gpu_mem_peak:.2f} MB\")\n",
    "else:\n",
    "    print(\"\\nGPU not available.\")\n",
    "\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97c1ad26-bf12-43e5-9cf4-754f01a3ff9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing volumes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [11:18<00:00,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "PSNR : 33.9939 ¬± 2.5453 dB\n",
      "NMSE (Mag, volume): 0.015349 ¬± 0.007232\n",
      "SSIM (Mag, slice):  0.8132 ¬± 0.0676\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
    "\n",
    "# # Path to validation folder\n",
    "# # val_folder = \"F:/denoised_preprocessed_h5_val\"\n",
    "\n",
    "# val_folder = r\"E:\\fastmri\\val_norm\"\n",
    "# # val_folder = r\"D:\\val_norm\"\n",
    "\n",
    "# # val_folder = r\"G:\\val_norm\\val_norm\"\n",
    "# # files = sorted([os.path.join(val_folder, f) for f in os.listdir(val_folder) if f.endswith(\".h5\")])\n",
    "# kspace_files_list_val = sorted(glob.glob(os.path.join(val_folder, \"*.h5\")))\n",
    "file_paths = kspace_files_list_val\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# HELPERS\n",
    "# ----------------------\n",
    "def to_complex(x):\n",
    "    return x[..., 0] + 1j * x[..., 1]\n",
    "\n",
    "def nmse(gt, pred):\n",
    "    return np.linalg.norm(gt - pred) ** 2 / (np.linalg.norm(gt) ** 2 + 1e-10)\n",
    "\n",
    "def compute_ssim(gt, pred, max_val):\n",
    "    return structural_similarity(\n",
    "        gt, pred,\n",
    "        data_range=max_val,\n",
    "        win_size=9,\n",
    "        gaussian_weights=False,\n",
    "        use_sample_covariance=False,\n",
    "        K1=0.01,\n",
    "        K2=0.03\n",
    "    )\n",
    "\n",
    "# ----------------------\n",
    "# STORAGE\n",
    "# ----------------------\n",
    "ssim_list = []\n",
    "psnr_list = []\n",
    "nmse_list = []\n",
    "\n",
    "# ----------------------\n",
    "# PROCESSING\n",
    "# ----------------------\n",
    "for file in tqdm(pd_files, desc=\"Processing volumes\"):\n",
    "    with h5py.File(file, 'r') as f:\n",
    "        kspace_under = f[\"kspace_under\"][:]   # (S, H, W, 2)\n",
    "        image_full   = f[\"image_full\"][:]     # (S, H, W, 2)\n",
    "        max_val = float(f[\"max_val_full_image\"][0])\n",
    "\n",
    "    num_slices = kspace_under.shape[0]\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # TILE MASK FOR THIS VOLUME\n",
    "    # --------------------------------------------------\n",
    "    mask_batch = np.tile(mask, (num_slices, 1, 1, 1))\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # MODEL INFERENCE\n",
    "    # --------------------------------------------------\n",
    "    pred = model.predict(\n",
    "        [kspace_under, mask_batch],\n",
    "        batch_size=1,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    \n",
    "    image_full *= max_val\n",
    "    \n",
    "    pred *= max_val  # Scale predicted output to original intensity range\n",
    "    #psnr_val = peak_signal_noise_ratio(image_full, pred, data_range=max_val)\n",
    "\n",
    "    # Convert to complex and get magnitude\n",
    "    gt_mag = np.abs(to_complex(image_full))\n",
    "    pred_mag = np.abs(to_complex(pred))\n",
    "\n",
    "    # Volume-wise PSNR and NMSE\n",
    "    psnr_val = peak_signal_noise_ratio(gt_mag, pred_mag, data_range=max_val)\n",
    "    nmse_val = nmse(gt_mag.flatten(), pred_mag.flatten())\n",
    "\n",
    "    psnr_list.append(psnr_val)\n",
    "    nmse_list.append(nmse_val)\n",
    "\n",
    "    # Slice-wise SSIM\n",
    "    for i in range(gt_mag.shape[0]):\n",
    "        ssim_val = compute_ssim(gt_mag[i], pred_mag[i], max_val)\n",
    "        ssim_list.append(ssim_val)\n",
    "\n",
    "# ----------------------\n",
    "# REPORT\n",
    "# ----------------------\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(f\"PSNR : {np.mean(psnr_list):.4f} ¬± {np.std(psnr_list):.4f} dB\")\n",
    "print(f\"NMSE (Mag, volume): {np.mean(nmse_list):.6f} ¬± {np.std(nmse_list):.6f}\")\n",
    "print(f\"SSIM (Mag, slice):  {np.mean(ssim_list):.4f} ¬± {np.std(ssim_list):.4f}\")\n",
    "\n",
    "print(\"=\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "772ce13f-be16-4ff5-b2b4-551ee0623df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "PSNR : 32.6425 ¬± 2.5847 dB\n",
      "NMSE (Mag, volume): 0.0293 ¬± 0.0128\n",
      "SSIM (Mag, slice):  0.7691 ¬± 0.0774\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(f\"PSNR : {np.mean(psnr_list):.4f} ¬± {np.std(psnr_list):.4f} dB\")\n",
    "print(f\"NMSE (Mag, volume): {np.mean(nmse_list):.4f} ¬± {np.std(nmse_list):.4f}\")\n",
    "print(f\"SSIM (Mag, slice):  {np.mean(ssim_list):.4f} ¬± {np.std(ssim_list):.4f}\")\n",
    "\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b275eae-d6c2-4a3a-a03e-f98d1bb1e188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PD volumes: 100\n",
      "PDFS volumes: 99\n"
     ]
    }
   ],
   "source": [
    "pd_files = []\n",
    "pdfs_files = []\n",
    "\n",
    "for f in kspace_files_list_val:\n",
    "    if \"PDFS\" in f:\n",
    "        pdfs_files.append(f)\n",
    "    else:\n",
    "        pd_files.append(f)\n",
    "\n",
    "print(f\"PD volumes: {len(pd_files)}\")\n",
    "print(f\"PDFS volumes: {len(pdfs_files)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00feebf4-2efc-4c5a-b8f4-b36e377b1ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing volumes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 199/199 [26:24<00:00,  7.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Evaluation complete\n",
      "Excel file saved to: validation_metrics_TEID_FI.xlsx\n",
      "==================================================\n",
      "PSNR : 33.3216 ¬± 2.6591 dB\n",
      "NMSE : 0.022312 ¬± 0.012549\n",
      "SSIM : 0.7913 ¬± 0.0759\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
    "\n",
    "# ======================================================\n",
    "# USER INPUTS (adjust if needed)\n",
    "# ======================================================\n",
    "\n",
    "val_folder = r\"D:\\fastmri_singlecoil_FSSCAN\\val_norm\"   # validation H5 folder\n",
    "output_excel = \"validation_metrics_TEID_FI.xlsx\"\n",
    "\n",
    "# model : your trained model (already loaded)\n",
    "# mask  : undersampling mask of shape (1, H, W, 1) or (1, H, W, 2)\n",
    "\n",
    "kspace_files_list_val = sorted(glob.glob(os.path.join(val_folder, \"*.h5\")))\n",
    "\n",
    "# ======================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ======================================================\n",
    "\n",
    "def to_complex(x):\n",
    "    \"\"\"Convert (..., 2) real/imag to complex\"\"\"\n",
    "    return x[..., 0] + 1j * x[..., 1]\n",
    "\n",
    "def nmse(gt, pred):\n",
    "    return np.linalg.norm(gt - pred) ** 2 / (np.linalg.norm(gt) ** 2 + 1e-10)\n",
    "\n",
    "def compute_ssim(gt, pred, max_val):\n",
    "    return structural_similarity(\n",
    "        gt,\n",
    "        pred,\n",
    "        data_range=max_val,\n",
    "        win_size=9,\n",
    "        gaussian_weights=False,\n",
    "        use_sample_covariance=False,\n",
    "        K1=0.01,\n",
    "        K2=0.03\n",
    "    )\n",
    "\n",
    "# ======================================================\n",
    "# METRIC STORAGE\n",
    "# ======================================================\n",
    "\n",
    "volume_metrics = []   # PSNR + NMSE (per volume)\n",
    "slice_metrics  = []   # SSIM (per slice)\n",
    "\n",
    "# ======================================================\n",
    "# PROCESSING LOOP\n",
    "# ======================================================\n",
    "\n",
    "for file in tqdm(kspace_files_list_val, desc=\"Processing volumes\"):\n",
    "    volume_name = os.path.basename(file)\n",
    "\n",
    "    with h5py.File(file, \"r\") as f:\n",
    "        kspace_under = f[\"kspace_under\"][:]      # (S, H, W, 2)\n",
    "        image_full   = f[\"image_full\"][:]        # (S, H, W, 2)\n",
    "        max_val      = float(f[\"max_val_full_image\"][0])\n",
    "\n",
    "    num_slices = kspace_under.shape[0]\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Mask tiling\n",
    "    # --------------------------------------------------\n",
    "    mask_batch = np.tile(mask, (num_slices, 1, 1, 1))\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Model inference\n",
    "    # --------------------------------------------------\n",
    "    pred = model.predict(\n",
    "        [kspace_under, mask_batch],\n",
    "        batch_size=1,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Rescale to original intensity\n",
    "    # --------------------------------------------------\n",
    "    image_full = image_full * max_val\n",
    "    pred       = pred * max_val\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Magnitude images\n",
    "    # --------------------------------------------------\n",
    "    gt_mag   = np.abs(to_complex(image_full))\n",
    "    pred_mag = np.abs(to_complex(pred))\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Volume-wise metrics\n",
    "    # --------------------------------------------------\n",
    "    psnr_val = peak_signal_noise_ratio(\n",
    "        gt_mag,\n",
    "        pred_mag,\n",
    "        data_range=max_val\n",
    "    )\n",
    "\n",
    "    nmse_val = nmse(\n",
    "        gt_mag.flatten(),\n",
    "        pred_mag.flatten()\n",
    "    )\n",
    "\n",
    "    volume_metrics.append({\n",
    "        \"volume_name\": volume_name,\n",
    "        \"num_slices\": num_slices,\n",
    "        \"PSNR_dB\": psnr_val,\n",
    "        \"NMSE\": nmse_val\n",
    "    })\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Slice-wise SSIM\n",
    "    # --------------------------------------------------\n",
    "    for i in range(num_slices):\n",
    "        ssim_val = compute_ssim(gt_mag[i], pred_mag[i], max_val)\n",
    "\n",
    "        slice_metrics.append({\n",
    "            \"volume_name\": volume_name,\n",
    "            \"slice_index\": i,\n",
    "            \"SSIM\": ssim_val\n",
    "        })\n",
    "\n",
    "# ======================================================\n",
    "# SAVE TO EXCEL\n",
    "# ======================================================\n",
    "\n",
    "df_volume = pd.DataFrame(volume_metrics)\n",
    "df_slice  = pd.DataFrame(slice_metrics)\n",
    "\n",
    "with pd.ExcelWriter(output_excel, engine=\"openpyxl\") as writer:\n",
    "    df_volume.to_excel(writer, sheet_name=\"Volume_Metrics_TEID_Final\", index=False)\n",
    "    df_slice.to_excel(writer, sheet_name=\"Slice_SSIM_Final\", index=False)\n",
    "\n",
    "# ======================================================\n",
    "# REPORT\n",
    "# ======================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Evaluation complete\")\n",
    "print(f\"Excel file saved to: {output_excel}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"PSNR : {df_volume['PSNR_dB'].mean():.4f} ¬± {df_volume['PSNR_dB'].std():.4f} dB\")\n",
    "print(f\"NMSE : {df_volume['NMSE'].mean():.6f} ¬± {df_volume['NMSE'].std():.6f}\")\n",
    "print(f\"SSIM : {df_slice['SSIM'].mean():.4f} ¬± {df_slice['SSIM'].std():.4f}\")\n",
    "print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2681e91c-6c7d-470b-9ca3-c1ccf7f05320",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
