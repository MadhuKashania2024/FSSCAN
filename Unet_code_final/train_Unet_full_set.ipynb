{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f31836e-ee71-4b87-a94a-16b623996312",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "\n",
    "# Importing callbacks and data augmentation utils\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import  Adam\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21f9d693-7d4e-4e48-90cc-547ad3ebe9dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\DU\\\\aman_fastmri\\\\Data\\\\mask_4x_320_random.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#hybrid_cascade/fastmri/Data/mask_4x_320_random.npy\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m var_sampling_mask\u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDU\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43maman_fastmri\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mData\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mmask_4x_320_random.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvar_sampling_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m,var_sampling_mask\u001b[38;5;241m.\u001b[39mshape,var_sampling_mask\u001b[38;5;241m.\u001b[39mdtype,\u001b[38;5;28mtype\u001b[39m(var_sampling_mask))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSampling:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m*\u001b[39mvar_sampling_mask\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m/\u001b[39mvar_sampling_mask\u001b[38;5;241m.\u001b[39msize)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\WNet\\lib\\site-packages\\numpy\\lib\\npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\DU\\\\aman_fastmri\\\\Data\\\\mask_4x_320_random.npy'"
     ]
    }
   ],
   "source": [
    "#hybrid_cascade/fastmri/Data/mask_4x_320_random.npy\n",
    "var_sampling_mask=np.load(r\"C:\\Users\\DU\\aman_fastmri\\Data\\mask_4x_320_random.npy\")\n",
    "print(\"var_sampling_mask\",var_sampling_mask.shape,var_sampling_mask.dtype,type(var_sampling_mask))\n",
    "\n",
    "\n",
    "print(\"Sampling:\", 1.0*var_sampling_mask.sum()/var_sampling_mask.size)\n",
    "num_zeros = np.sum(var_sampling_mask[0] == 0)\n",
    "num_ones = np.sum(var_sampling_mask[0] == 1)\n",
    "# Print results\n",
    "total_pixels = var_sampling_mask[0].size\n",
    "\n",
    "# Calculate percentages\n",
    "ones_percentage = (num_ones / total_pixels) * 100\n",
    "zeros_percentage = (num_zeros / total_pixels) * 100\n",
    "\n",
    "print(f\"Number of 1s: {num_ones} ({ones_percentage:.8f}%)\")\n",
    "print(f\"Number of 0s: {num_zeros} ({zeros_percentage:.8f}%)\")\n",
    "print(\"Min\",var_sampling_mask[0].min())\n",
    "print(\"Max\",var_sampling_mask[0].max())\n",
    "print(\"data range\",var_sampling_mask[0].max()-var_sampling_mask[0].min())\n",
    "print(\"var_sampling_mask\",var_sampling_mask.shape,var_sampling_mask.dtype,type(var_sampling_mask))\n",
    "print(\"var_sampling_mask[0]\",var_sampling_mask[0].shape,var_sampling_mask[0].dtype,type(var_sampling_mask[0]))\n",
    "plt.imshow(var_sampling_mask[0],cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "297f9d98-18e3-4e74-97f6-ad4501e91afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "class VolumeWiseNMSE(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, val_files, save_best_path):\n",
    "        self.val_files = val_files\n",
    "        self.best_nmse = float('inf')\n",
    "        self.save_best_path = save_best_path\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        total_nmse = 0\n",
    "        num_volumes = 0\n",
    "\n",
    "        for file_path in self.val_files:\n",
    "            with h5py.File(file_path, 'r') as f:\n",
    "                image_under = f['image_under'][:]  # [slices, H, W, 2]\n",
    "                gt = f['image_full'][:]\n",
    "\n",
    "            pred = []\n",
    "            for i in range(image_under.shape[0]):\n",
    "                input_slice = np.expand_dims(image_under[i], axis=0)\n",
    "                pred_slice = self.model.predict(input_slice, verbose=0)\n",
    "                pred.append(pred_slice[0])\n",
    "            pred = np.stack(pred)\n",
    "\n",
    "            nmse = np.sum((np.abs(gt - pred) ** 2)) / np.sum((np.abs(gt) ** 2))\n",
    "            total_nmse += nmse\n",
    "            num_volumes += 1\n",
    "\n",
    "        avg_nmse = total_nmse / num_volumes\n",
    "        print(f\"\\nEpoch {epoch + 1} - Avg NMSE: {avg_nmse:.6f}\")\n",
    "\n",
    "        if avg_nmse < self.best_nmse:\n",
    "            print(f\"New best model found! Saving model with NMSE {avg_nmse:.6f}\")\n",
    "            self.best_nmse = avg_nmse\n",
    "            self.model.save(self.save_best_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ab92fc1-786a-4dd4-be26-8b3c870808dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = r\"E:\\fastmri_singlecoil_FSSCAN\\train_norm\"\n",
    "val_folder = r\"E:\\fastmri_singlecoil_FSSCAN\\val_norm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a7bd245-5bb2-4064-b0ee-951351dbb4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRISliceGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, file_list, batch_size=4, shuffle=True):\n",
    "        self.file_list = file_list\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.slice_index_map = []  # list of (file_idx, slice_idx)\n",
    "        self._build_index()\n",
    "\n",
    "    def _build_index(self):\n",
    "        for file_idx, file_path in enumerate(self.file_list):\n",
    "            with h5py.File(file_path, 'r') as f:\n",
    "                num_slices = f['image_under'].shape[0]\n",
    "                for slice_idx in range(num_slices):\n",
    "                    self.slice_index_map.append((file_idx, slice_idx))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.slice_index_map) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_map = self.slice_index_map[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_x, batch_y = [], []\n",
    "\n",
    "        for file_idx, slice_idx in batch_map:\n",
    "            with h5py.File(self.file_list[file_idx], 'r') as f:\n",
    "                x = f['image_under'][slice_idx]  # [H, W, 2]\n",
    "                y = f['image_full'][slice_idx]   # [H, W, 2]\n",
    "\n",
    "                #y_norm = complex_zscore(y)\n",
    "\n",
    "                batch_x.append(x)\n",
    "                batch_y.append(y)\n",
    "\n",
    "        return np.array(batch_x), np.array(batch_y)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.slice_index_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa6142e5-dae8-4876-ae55-1cc321d2ca26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1784\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "#kspace_files_list_train = sorted(glob.glob(os.path.join(train_folder, \"*.h5\")))\n",
    "kspace_files_list_val = sorted(glob.glob(os.path.join(val_folder, \"*.h5\")))\n",
    "\n",
    "# half_train = 20\n",
    "# half_val = 10\n",
    "#half_train = len(kspace_files_list_train) \n",
    "half_val = len(kspace_files_list_val) \n",
    "# print(\"half_train\",half_train)\n",
    "# print(\"half_val\",half_val)\n",
    "#kspace_files_list_train = kspace_files_list_train[:]\n",
    "kspace_files_list_val = kspace_files_list_val[:]\n",
    "\n",
    "# Create generators\n",
    "# train_gen = MRISliceGenerator(kspace_files_list_train,batch_size=16, shuffle=True,mask=mask)\n",
    "# val_gen = MRISliceGenerator(kspace_files_list_val, batch_size=4, shuffle=False,mask=mask)\n",
    "#train_gen = MRISliceGenerator(kspace_files_list_train,batch_size=8, shuffle=True)\n",
    "val_gen = MRISliceGenerator(kspace_files_list_val, batch_size=4, shuffle=False)\n",
    "\n",
    "#print(len(train_gen))  \n",
    "print(len(val_gen))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9dd550-fad5-4551-9f02-a7cf1ac11baa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "732d783b-ea19-45d3-aace-4695a3943de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run model_unet.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "752bff35-d8ec-4ad9-a03a-cabb6649b2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import os\n",
    "\n",
    "# # Define the log directory\n",
    "# log_dir = \"./logs/fastmri_Unet_4x_mae_full_val_nmse\"\n",
    "\n",
    "# # Create the TensorBoard callback\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c34c853-f0c9-4d78-84a0-cbde5887495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def nmse(y_true, y_pred):\n",
    "    return K.mean(K.square(y_true - y_pred)) / K.mean(K.square(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bec4eda2-42a1-4ef8-ac19-e198aea3b61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "\n",
    "\n",
    "# Define learning rate schedule\n",
    "def lr_schedule(epoch):\n",
    "    if epoch < 40:\n",
    "        return 0.001\n",
    "    else:\n",
    "        return 0.0001  # Multiply by 0.1 after 40 epochs\n",
    "epochs = 50\n",
    "batch_size=16\n",
    "\n",
    "model_name = \"./SavedModels_FULL_SET/Unet_4x_320_mae_full_nmse.hdf5\"\n",
    "\n",
    "H, W = 320, 320\n",
    "model = unet(H=H, W=W, channels=2, kshape=(3, 3))\n",
    "opt = RMSprop(learning_rate=0.001)\n",
    "\n",
    "model.compile(loss='mae', optimizer=opt, metrics=[nmse])\n",
    "\n",
    "# Load weights if available\n",
    "if os.path.isfile(model_name):\n",
    "    model.load_weights(model_name)\n",
    "    print(\"Weights loaded\")\n",
    "\n",
    "# Define callbacks\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "checkpoint = ModelCheckpoint(model_name, monitor='val_loss', save_best_only=True, mode='min')\n",
    "# Early stopping callback to shut down training after\n",
    "# 5 epochs with no improvement\n",
    "#earlyStopping = EarlyStopping(monitor='val_loss',patience=20, verbose=0, mode='min')\n",
    "\n",
    "\n",
    "if os.path.isfile(model_name):\n",
    "    model.load_weights(model_name)\n",
    "    print(\"weights loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "544ca804-130e-48ed-8e14-a062d14bc090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "H, W = 320, 320\n",
    "model = unet(H=H, W=W, channels=2, kshape=(3, 3))\n",
    "opt = RMSprop(learning_rate=0.001)\n",
    "\n",
    "model_name = \"./SavedModels_FULL_SET/best_model_nmse_full_volume.h5\"\n",
    "model.compile(loss='mae', optimizer=opt)\n",
    "\n",
    "# Load weights if available\n",
    "if os.path.isfile(model_name):\n",
    "    model.load_weights(model_name)\n",
    "    print(\"Weights loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce5ccd62-9be0-45d9-8137-782a4412aaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0307      \n",
      "Epoch 1 - Avg NMSE: 0.083539\n",
      "New best model found! Saving model with NMSE 0.083539\n",
      "4338/4338 [==============================] - 2407s 553ms/step - loss: 0.0307 - val_loss: 0.0233\n",
      "Epoch 2/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0212   \n",
      "Epoch 2 - Avg NMSE: 0.073690\n",
      "New best model found! Saving model with NMSE 0.073690\n",
      "4338/4338 [==============================] - 2447s 564ms/step - loss: 0.0212 - val_loss: 0.0217\n",
      "Epoch 3/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0203   \n",
      "Epoch 3 - Avg NMSE: 0.066867\n",
      "New best model found! Saving model with NMSE 0.066867\n",
      "4338/4338 [==============================] - 2490s 574ms/step - loss: 0.0203 - val_loss: 0.0202\n",
      "Epoch 4/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0199   \n",
      "Epoch 4 - Avg NMSE: 0.066396\n",
      "New best model found! Saving model with NMSE 0.066396\n",
      "4338/4338 [==============================] - 2515s 580ms/step - loss: 0.0199 - val_loss: 0.0201\n",
      "Epoch 5/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0197   \n",
      "Epoch 5 - Avg NMSE: 0.067237\n",
      "4338/4338 [==============================] - 2595s 598ms/step - loss: 0.0197 - val_loss: 0.0203\n",
      "Epoch 6/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0196   \n",
      "Epoch 6 - Avg NMSE: 0.068613\n",
      "4338/4338 [==============================] - 2697s 622ms/step - loss: 0.0196 - val_loss: 0.0205\n",
      "Epoch 7/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0195   \n",
      "Epoch 7 - Avg NMSE: 0.068019\n",
      "4338/4338 [==============================] - 2793s 644ms/step - loss: 0.0195 - val_loss: 0.0206\n",
      "Epoch 8/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0194   \n",
      "Epoch 8 - Avg NMSE: 0.070567\n",
      "4338/4338 [==============================] - 2892s 667ms/step - loss: 0.0194 - val_loss: 0.0209\n",
      "Epoch 9/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0193   \n",
      "Epoch 9 - Avg NMSE: 0.064541\n",
      "New best model found! Saving model with NMSE 0.064541\n",
      "4338/4338 [==============================] - 3001s 692ms/step - loss: 0.0193 - val_loss: 0.0197\n",
      "Epoch 10/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0193   \n",
      "Epoch 10 - Avg NMSE: 0.064300\n",
      "New best model found! Saving model with NMSE 0.064300\n",
      "4338/4338 [==============================] - 3190s 735ms/step - loss: 0.0193 - val_loss: 0.0197\n",
      "Epoch 11/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0192     \n",
      "Epoch 11 - Avg NMSE: 0.066371\n",
      "4338/4338 [==============================] - 3401s 784ms/step - loss: 0.0192 - val_loss: 0.0200\n",
      "Epoch 12/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0192   \n",
      "Epoch 12 - Avg NMSE: 0.066344\n",
      "4338/4338 [==============================] - 3668s 846ms/step - loss: 0.0192 - val_loss: 0.0200\n",
      "Epoch 13/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0191   \n",
      "Epoch 13 - Avg NMSE: 0.064122\n",
      "New best model found! Saving model with NMSE 0.064122\n",
      "4338/4338 [==============================] - 3878s 894ms/step - loss: 0.0191 - val_loss: 0.0197\n",
      "Epoch 14/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0191   \n",
      "Epoch 14 - Avg NMSE: 0.065105\n",
      "4338/4338 [==============================] - 4085s 942ms/step - loss: 0.0191 - val_loss: 0.0198\n",
      "Epoch 15/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0191     \n",
      "Epoch 15 - Avg NMSE: 0.068756\n",
      "4338/4338 [==============================] - 4264s 983ms/step - loss: 0.0191 - val_loss: 0.0207\n",
      "Epoch 16/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0190   \n",
      "Epoch 16 - Avg NMSE: 0.065461\n",
      "4338/4338 [==============================] - 4467s 1s/step - loss: 0.0190 - val_loss: 0.0199\n",
      "Epoch 17/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0190     \n",
      "Epoch 17 - Avg NMSE: 0.063855\n",
      "New best model found! Saving model with NMSE 0.063855\n",
      "4338/4338 [==============================] - 4652s 1s/step - loss: 0.0190 - val_loss: 0.0196\n",
      "Epoch 18/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0190     \n",
      "Epoch 18 - Avg NMSE: 0.063846\n",
      "New best model found! Saving model with NMSE 0.063846\n",
      "4338/4338 [==============================] - 4877s 1s/step - loss: 0.0190 - val_loss: 0.0196\n",
      "Epoch 19/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0190     \n",
      "Epoch 19 - Avg NMSE: 0.063904\n",
      "4338/4338 [==============================] - 5040s 1s/step - loss: 0.0190 - val_loss: 0.0196\n",
      "Epoch 20/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0190     \n",
      "Epoch 20 - Avg NMSE: 0.063555\n",
      "New best model found! Saving model with NMSE 0.063555\n",
      "4338/4338 [==============================] - 5286s 1s/step - loss: 0.0190 - val_loss: 0.0196\n",
      "Epoch 21/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0190     \n",
      "Epoch 21 - Avg NMSE: 0.065263\n",
      "4338/4338 [==============================] - 5548s 1s/step - loss: 0.0190 - val_loss: 0.0198\n",
      "Epoch 22/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0189     \n",
      "Epoch 22 - Avg NMSE: 0.063798\n",
      "4338/4338 [==============================] - 5746s 1s/step - loss: 0.0189 - val_loss: 0.0196\n",
      "Epoch 23/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0189     \n",
      "Epoch 23 - Avg NMSE: 0.065537\n",
      "4338/4338 [==============================] - 6291s 1s/step - loss: 0.0189 - val_loss: 0.0200\n",
      "Epoch 24/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0189     \n",
      "Epoch 24 - Avg NMSE: 0.066523\n",
      "4338/4338 [==============================] - 6479s 1s/step - loss: 0.0189 - val_loss: 0.0201\n",
      "Epoch 25/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0189     \n",
      "Epoch 25 - Avg NMSE: 0.065437\n",
      "4338/4338 [==============================] - 7340s 2s/step - loss: 0.0189 - val_loss: 0.0200\n",
      "Epoch 26/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0189     \n",
      "Epoch 26 - Avg NMSE: 0.063520\n",
      "New best model found! Saving model with NMSE 0.063520\n",
      "4338/4338 [==============================] - 7529s 2s/step - loss: 0.0189 - val_loss: 0.0195\n",
      "Epoch 27/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0189     \n",
      "Epoch 27 - Avg NMSE: 0.065218\n",
      "4338/4338 [==============================] - 7857s 2s/step - loss: 0.0189 - val_loss: 0.0199\n",
      "Epoch 28/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0189     \n",
      "Epoch 28 - Avg NMSE: 0.065486\n",
      "4338/4338 [==============================] - 8368s 2s/step - loss: 0.0189 - val_loss: 0.0199\n",
      "Epoch 29/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0189     \n",
      "Epoch 29 - Avg NMSE: 0.063359\n",
      "New best model found! Saving model with NMSE 0.063359\n",
      "4338/4338 [==============================] - 8497s 2s/step - loss: 0.0189 - val_loss: 0.0195\n",
      "Epoch 30/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0188     \n",
      "Epoch 30 - Avg NMSE: 0.064320\n",
      "4338/4338 [==============================] - 9368s 2s/step - loss: 0.0188 - val_loss: 0.0198\n",
      "Epoch 31/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0188     \n",
      "Epoch 31 - Avg NMSE: 0.067244\n",
      "4338/4338 [==============================] - 9290s 2s/step - loss: 0.0188 - val_loss: 0.0203\n",
      "Epoch 32/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0188     \n",
      "Epoch 32 - Avg NMSE: 0.065094\n",
      "4338/4338 [==============================] - 9742s 2s/step - loss: 0.0188 - val_loss: 0.0199\n",
      "Epoch 33/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0188     \n",
      "Epoch 33 - Avg NMSE: 0.063607\n",
      "4338/4338 [==============================] - 9481s 2s/step - loss: 0.0188 - val_loss: 0.0196\n",
      "Epoch 34/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0188     \n",
      "Epoch 34 - Avg NMSE: 0.064235\n",
      "4338/4338 [==============================] - 9981s 2s/step - loss: 0.0188 - val_loss: 0.0197\n",
      "Epoch 35/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0188     \n",
      "Epoch 35 - Avg NMSE: 0.063945\n",
      "4338/4338 [==============================] - 10601s 2s/step - loss: 0.0188 - val_loss: 0.0196\n",
      "Epoch 36/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0188     \n",
      "Epoch 36 - Avg NMSE: 0.064730\n",
      "4338/4338 [==============================] - 11790s 3s/step - loss: 0.0188 - val_loss: 0.0197\n",
      "Epoch 37/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0188     \n",
      "Epoch 37 - Avg NMSE: 0.064037\n",
      "4338/4338 [==============================] - 11355s 3s/step - loss: 0.0188 - val_loss: 0.0196\n",
      "Epoch 38/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0188     \n",
      "Epoch 38 - Avg NMSE: 0.064758\n",
      "4338/4338 [==============================] - 12094s 3s/step - loss: 0.0188 - val_loss: 0.0199\n",
      "Epoch 39/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0188     \n",
      "Epoch 39 - Avg NMSE: 0.064390\n",
      "4338/4338 [==============================] - 12646s 3s/step - loss: 0.0188 - val_loss: 0.0197\n",
      "Epoch 40/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0188     \n",
      "Epoch 40 - Avg NMSE: 0.064162\n",
      "4338/4338 [==============================] - 12207s 3s/step - loss: 0.0188 - val_loss: 0.0196\n",
      "\n",
      "Epoch 41: Reducing learning rate from 0.0010000000474974513 to 0.00010000000474974513\n",
      "Epoch 41/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0183     \n",
      "Epoch 41 - Avg NMSE: 0.063231\n",
      "New best model found! Saving model with NMSE 0.063231\n",
      "4338/4338 [==============================] - 13508s 3s/step - loss: 0.0183 - val_loss: 0.0194\n",
      "Epoch 42/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0183     \n",
      "Epoch 42 - Avg NMSE: 0.063199\n",
      "New best model found! Saving model with NMSE 0.063199\n",
      "4338/4338 [==============================] - 14183s 3s/step - loss: 0.0183 - val_loss: 0.0194\n",
      "Epoch 43/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0183     \n",
      "Epoch 43 - Avg NMSE: 0.063198\n",
      "New best model found! Saving model with NMSE 0.063198\n",
      "4338/4338 [==============================] - 14803s 3s/step - loss: 0.0183 - val_loss: 0.0194\n",
      "Epoch 44/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0183     \n",
      "Epoch 44 - Avg NMSE: 0.063190\n",
      "New best model found! Saving model with NMSE 0.063190\n",
      "4338/4338 [==============================] - 15316s 4s/step - loss: 0.0183 - val_loss: 0.0194\n",
      "Epoch 45/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0183     \n",
      "Epoch 45 - Avg NMSE: 0.063233\n",
      "4338/4338 [==============================] - 15864s 4s/step - loss: 0.0183 - val_loss: 0.0194\n",
      "Epoch 46/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0183     \n",
      "Epoch 46 - Avg NMSE: 0.063316\n",
      "4338/4338 [==============================] - 14986s 3s/step - loss: 0.0183 - val_loss: 0.0194\n",
      "Epoch 47/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0183     \n",
      "Epoch 47 - Avg NMSE: 0.063144\n",
      "New best model found! Saving model with NMSE 0.063144\n",
      "4338/4338 [==============================] - 15388s 4s/step - loss: 0.0183 - val_loss: 0.0194\n",
      "Epoch 48/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0183     \n",
      "Epoch 48 - Avg NMSE: 0.063239\n",
      "4338/4338 [==============================] - 16407s 4s/step - loss: 0.0183 - val_loss: 0.0194\n",
      "Epoch 49/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0183     \n",
      "Epoch 49 - Avg NMSE: 0.063224\n",
      "4338/4338 [==============================] - 17874s 4s/step - loss: 0.0183 - val_loss: 0.0194\n",
      "Epoch 50/50\n",
      "4338/4338 [==============================] - ETA: 0s - loss: 0.0183     \n",
      "Epoch 50 - Avg NMSE: 0.063298\n",
      "4338/4338 [==============================] - 17689s 4s/step - loss: 0.0183 - val_loss: 0.0194\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fb96c80550>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nmse_callback = VolumeWiseNMSE(kspace_files_list_val, save_best_path=model_name)\n",
    "\n",
    "class StepDecayLR(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch == 40:\n",
    "            old_lr = float(tf.keras.backend.get_value(self.model.optimizer.lr))\n",
    "            new_lr = old_lr * 0.1\n",
    "            tf.keras.backend.set_value(self.model.optimizer.lr, new_lr)\n",
    "            print(f\"\\nEpoch {epoch + 1}: Reducing learning rate from {old_lr} to {new_lr}\")\n",
    "\n",
    "model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=50,\n",
    "    callbacks=[nmse_callback, StepDecayLR()]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90021f66-c55e-4220-b362-ef64ac8983ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt  # <-- Required for plotting\n",
    "from typing import Optional\n",
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
    "from tqdm import tqdm\n",
    "def psnr(\n",
    "    gt: np.ndarray, pred: np.ndarray, maxval: Optional[float] = None\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Compute Peak Signal to Noise Ratio metric (PSNR)\"\"\"\n",
    "    if maxval is None:\n",
    "        maxval = gt.max()\n",
    "    return peak_signal_noise_ratio(gt, pred, data_range=maxval)\n",
    "\n",
    "\n",
    "# Your preprocessed folder\n",
    "save_dir = r\"D:\\fastmri\\preprocessed_h5_val\"\n",
    "files = sorted([os.path.join(save_dir, f) for f in os.listdir(save_dir) if f.endswith('.h5')])\n",
    "psnr_all_volumes = []\n",
    "\n",
    "for file in files:\n",
    "    with h5py.File(file, 'r') as f:\n",
    "        image_full = f['image_full'][:]   # shape: [slices, H, W, 2]\n",
    "        image_under = f['image_under'][:]\n",
    "        max_val=f['max_val_full'][:]\n",
    "\n",
    "    # Convert to complex\n",
    "    #image_full_complex = image_full[..., 0] + 1j * image_full[..., 1]\n",
    "    #image_under_complex = image_under[..., 0] + 1j * image_under[..., 1]\n",
    "\n",
    "    # Convert to magnitude\n",
    "    #mag_gt = np.abs(image_full_complex)\n",
    "    #mag_under = np.abs(image_under_complex)\n",
    "    psnrs = psnr(image_full,image_under,max_val)\n",
    "    #mean_psnr = np.mean(psnrs)\n",
    "\n",
    "    psnr_all_volumes.append(psnrs)\n",
    "    #print(f\"{os.path.basename(file)}: PSNR = {mean_psnr:.2f} dB\")\n",
    "# Overall average and standard deviation\n",
    "overall_avg_psnr = np.mean(psnr_all_volumes)\n",
    "overall_std_psnr = np.std(psnr_all_volumes)\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"Overall Average PSNR: {overall_avg_psnr:.6f} ± {overall_std_psnr:.6f} dB\")\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88ece3af-b596-41ca-95db-4395d5e32366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import os\n",
    "\n",
    "import math\n",
    " # <-- Required for plotting\n",
    "from typing import Optional\n",
    "from skimage.metrics import peak_signal_noise_ratio\n",
    "\n",
    "from skimage.metrics import structural_similarity as compare_ssim\n",
    "from tqdm import tqdm\n",
    "\n",
    "def psnr(\n",
    "    gt: np.ndarray, pred: np.ndarray, maxval: Optional[float] = None\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Compute Peak Signal to Noise Ratio metric (PSNR)\"\"\"\n",
    "    if maxval is None:\n",
    "        maxval = gt.max()\n",
    "    return peak_signal_noise_ratio(gt, pred, data_range=maxval)\n",
    "\n",
    "def nmse(gt: np.ndarray, pred: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute Normalized Mean Squared Error (NMSE) over full volume.\"\"\"\n",
    "    return np.linalg.norm(gt - pred) ** 2 / np.linalg.norm(gt) ** 2\n",
    "\n",
    "#\n",
    "def visualize_and_save_all_slices(model, val_files):\n",
    "    #os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    all_volume_psnrs = []\n",
    "    nmse_all_volumes = []\n",
    "    ssim_all_volumes=[]\n",
    "\n",
    "    for file_path in val_files:\n",
    "        volume_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        #pdf_path = os.path.join(output_dir, f\"{volume_name}.pdf\")\n",
    "\n",
    "        with h5py.File(file_path, 'r') as f:\n",
    "            image_under = f['image_under'][:]  # [slices, H, W, 2]\n",
    "            gt = f['image_full'][:]            # [slices, H, W, 2]\n",
    "\n",
    "        pred = []\n",
    "        for i in range(image_under.shape[0]):\n",
    "            input_slice = np.expand_dims(image_under[i], axis=0)  # [1, H, W, 2]\n",
    "            pred_slice = model.predict(input_slice, verbose=0)\n",
    "            pred.append(pred_slice[0])\n",
    "        pred = np.stack(pred)  # [slices, H, W, 2]\n",
    "\n",
    "        # Volume-wise PSNR using all channels\n",
    "        volume_psnr = psnr(gt, pred)\n",
    "        all_volume_psnrs.append(volume_psnr)\n",
    "\n",
    "        # Convert complex for visualization (abs)\n",
    "        gt_complex = gt[..., 0] + 1j * gt[..., 1]\n",
    "        pred_complex = pred[..., 0] + 1j * pred[..., 1]\n",
    "        under_complex = image_under[..., 0] + 1j * image_under[..., 1]\n",
    "\n",
    "        mag_gt = np.abs(gt_complex)\n",
    "        mag_pred = np.abs(pred_complex)\n",
    "        mag_under = np.abs(under_complex)\n",
    "                # Compute NMSE over full volume (flattened)\n",
    "        nmse_volume = nmse(mag_gt.flatten(), mag_pred.flatten())\n",
    "    \n",
    "        nmse_all_volumes.append(nmse_volume)\n",
    "\n",
    "        \n",
    "        max_val=np.max(mag_gt)\n",
    "    \n",
    "        ssim_slices = []\n",
    "        for i in range(mag_gt.shape[0]):\n",
    "            gt_mag = mag_gt[i]\n",
    "            pred_mag = mag_pred[i]\n",
    "    \n",
    "            ssim_slice= compare_ssim(\n",
    "                gt_mag, pred_mag,\n",
    "                data_range=max_val,\n",
    "                win_size=7,\n",
    "                gaussian_weights=False,\n",
    "                use_sample_covariance=False,      # <-- now correct per paper\n",
    "                K1=0.01,\n",
    "                K2=0.03,\n",
    "                full=False\n",
    "            )\n",
    "            ssim_slices.append(ssim_slice)\n",
    "    \n",
    "        ssim_volume = np.mean(ssim_slices)\n",
    "        ssim_all_volumes.append(ssim_volume)\n",
    "\n",
    "        #print(ssim_volume)\n",
    "        #print()\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "    overall_avg_psnr = np.mean(all_volume_psnrs)\n",
    "    overall_std_psnr = np.std(all_volume_psnrs)\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(f\"Overall Average PSNR: {overall_avg_psnr:.6f} ± {overall_std_psnr:.6f} dB\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    \n",
    "    overall_avg_nmse = np.mean(nmse_all_volumes)\n",
    "    overall_std_nmse = np.std(nmse_all_volumes)\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(f\"Overall Average NMSE: {overall_avg_nmse:.6f} ± {overall_std_nmse:.6f} dB\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    overall_avg_ssim = np.mean(ssim_all_volumes)\n",
    "    overall_std_ssim = np.std(ssim_all_volumes)\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(f\"Overall Average SSIM: {overall_avg_ssim:.6f} ± {overall_std_ssim:.6f} dB\")\n",
    "    print(\"=\"*40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0b25c40-947f-46c6-9122-5f84389ab911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Overall Average PSNR: 32.104779 ± 3.766329 dB\n",
      "========================================\n",
      "\n",
      "========================================\n",
      "Overall Average NMSE: 0.021325 ± 0.012479 dB\n",
      "========================================\n",
      "\n",
      "========================================\n",
      "Overall Average SSIM: 0.788387 ± 0.077827 dB\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output_dir = \"./validation_results\"\n",
    "visualize_and_save_all_slices(model, kspace_files_list_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb859b21-f69c-4345-8e35-ca5eed9b2952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f17c90a-d3b1-4fde-8ba2-ab14071d48a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
