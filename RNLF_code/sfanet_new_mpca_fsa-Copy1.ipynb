{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "547fde1c-5b75-4c1e-8101-62d377a83ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fe499da-e53d-40d9-b5da-69e72d83e066",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveGlobalFilter(tf.keras.layers.Layer):\n",
    "    def __init__(self, ratio=10):\n",
    "        super().__init__()\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # input_shape: [B, H, W, C, 2]\n",
    "        _, H, W, C, _ = input_shape\n",
    "        self.H, self.W, self.C = H, W, C\n",
    "\n",
    "        # initializer = tf.keras.initializers.RandomNormal(stddev=0.02)\n",
    "        self.filter = self.add_weight(\n",
    "            shape=(C, H, W, 2),\n",
    "            # initializer=initializer,\n",
    "            trainable=True,\n",
    "            name='complex_filter'\n",
    "        )\n",
    "\n",
    "        self.mask_low = self._create_static_mask()\n",
    "        self.mask_high = 1.0 - self.mask_low\n",
    "\n",
    "    def _create_static_mask(self):\n",
    "        mask = tf.zeros((self.H, self.W), dtype=tf.float32)\n",
    "        crow, ccol = self.H // 2, self.W // 2\n",
    "        r = self.ratio\n",
    "\n",
    "        indices = tf.stack(tf.meshgrid(\n",
    "            tf.range(crow - r, crow + r),\n",
    "            tf.range(ccol - r, ccol + r),\n",
    "            indexing='ij'\n",
    "        ), axis=-1)\n",
    "        indices = tf.reshape(indices, [-1, 2])\n",
    "        updates = tf.ones((tf.shape(indices)[0],), dtype=tf.float32)\n",
    "\n",
    "        return tf.Variable(tf.tensor_scatter_nd_update(mask, indices, updates), trainable=False)\n",
    "\n",
    "    def call(self, x):\n",
    "        tf.debugging.check_numerics(x, \"Input to AGF has NaNs/Infs\")\n",
    "        # x: [B, H, W, C, 2]\n",
    "        x = tf.transpose(x, [0, 3, 1, 2, 4])  # [B, C, H, W, 2]\n",
    "        x_freq = fft2c_tf(x)                 # [B, C, H, W, 2]\n",
    "        tf.debugging.check_numerics(x_freq, \"FFT2 output has NaNs/Infs\")\n",
    "\n",
    "\n",
    "        # Convert learnable filter to complex\n",
    "        real_part = self.filter[..., 0]\n",
    "        imag_part = self.filter[..., 1]\n",
    "        weight = tf.complex(real_part, imag_part)  # [C, H, W]\n",
    "        weight = tf.expand_dims(weight, axis=0)    # [1, C, H, W]\n",
    "\n",
    "        # Cast masks\n",
    "        mask_low_c = tf.cast(tf.reshape(self.mask_low, [1, 1, self.H, self.W]), tf.complex64)\n",
    "        mask_high_c = tf.cast(tf.reshape(self.mask_high, [1, 1, self.H, self.W]), tf.complex64)\n",
    "\n",
    "        # Convert x_freq to complex\n",
    "        x_freq_c = tf.complex(x_freq[..., 0], x_freq[..., 1])  # [B, C, H, W]\n",
    "\n",
    "        x_low = x_freq_c * mask_low_c\n",
    "        x_high = x_freq_c * mask_high_c\n",
    "\n",
    "        x_low_filtered = x_low * weight\n",
    "        x_combined = x_low_filtered + x_high  # [B, C, H, W]\n",
    "\n",
    "        x_combined = tf.stack([tf.math.real(x_combined), tf.math.imag(x_combined)], axis=-1)  # [B, C, H, W, 2]\n",
    "        x_combined = tf.transpose(x_combined, [0, 2, 3, 1, 4])  # [B, H, W, C, 2]\n",
    "\n",
    "        # Apply IFFT\n",
    "        x_out = ifft2c_tf(x_combined)  # [B, H, W, C, 2]\n",
    "        tf.debugging.check_numerics(x_out, \"IFFT2 output has NaNs/Infs\")\n",
    "        return x_out\n",
    "\n",
    "# # # Example dummy input: [batch, height, width, channels]\n",
    "# dummy_input = tf.random.normal((2, 512, 512, 64,2))\n",
    "\n",
    "# # Instantiate layer\n",
    "# agf = AdaptiveGlobalFilter(ratio=10)\n",
    "\n",
    "# # Forward pass\n",
    "# output = agf(dummy_input)\n",
    "\n",
    "# print(\"Input shape: \", dummy_input.shape)\n",
    "# print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9a776b5-cb8c-40bd-98bd-6ec82b02c6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class SpatialAttention(tf.keras.layers.Layer):\n",
    "#     def __init__(self):\n",
    "#         super(SpatialAttention, self).__init__()\n",
    "#         self.conv = tf.keras.layers.Conv2D(\n",
    "#             filters=1,\n",
    "#             kernel_size=7,\n",
    "#             padding='same',\n",
    "#             use_bias=False\n",
    "#         )\n",
    "#         self.sigmoid = tf.keras.activations.sigmoid\n",
    "\n",
    "#     def call(self, x):\n",
    "#         # x: [B, H, W, C, 2]\n",
    "#         real = x[..., 0]\n",
    "#         imag = x[..., 1]\n",
    "#         mag = tf.sqrt(real**2 + imag**2 + 1e-6)    # add eps for stability\n",
    "#         avg_out = tf.reduce_mean(mag, axis=-1, keepdims=True)  # [B,H,W,1]\n",
    "#         max_out = tf.reduce_max(mag, axis=-1, keepdims=True)   # [B,H,W,1]\n",
    "#         concat = tf.concat([avg_out, max_out], axis=-1)        # [B,H,W,2]\n",
    "#         attn_map = self.sigmoid(self.conv(concat))             # [B,H,W,1]\n",
    "#         attn_map = tf.expand_dims(attn_map, axis=-1)           # [B,H,W,1,1]\n",
    "#         return x * attn_map                                    # broadcast to [B,H,W,C,2]\n",
    "\n",
    "\n",
    "class SpatialAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv = tf.keras.layers.Conv2D(\n",
    "            filters=1,\n",
    "            kernel_size=7,\n",
    "            padding='same',\n",
    "            use_bias=False\n",
    "        )\n",
    "        self.sigmoid = tf.keras.activations.sigmoid\n",
    "        self.last_attn_map = None  # Optional: for saving the attention map\n",
    "\n",
    "    def call(self, x, return_attn=False):\n",
    "        # x: [B, H, W, C, 2]\n",
    "        real = x[..., 0]\n",
    "        imag = x[..., 1]\n",
    "        mag = tf.sqrt(real**2 + imag**2 + 1e-6)    # Magnitude for attention\n",
    "\n",
    "        avg_out = tf.reduce_mean(mag, axis=-1, keepdims=True)  # [B, H, W, 1]\n",
    "        max_out = tf.reduce_max(mag, axis=-1, keepdims=True)   # [B, H, W, 1]\n",
    "        concat = tf.concat([avg_out, max_out], axis=-1)        # [B, H, W, 2]\n",
    "\n",
    "        attn_map = self.sigmoid(self.conv(concat))             # [B, H, W, 1]\n",
    "        attn_map = tf.expand_dims(attn_map, axis=-1)           # [B, H, W, 1, 1]\n",
    "\n",
    "        self.last_attn_map = attn_map  # Optionally store for later\n",
    "\n",
    "        output = x * attn_map  # Broadcasting: [B, H, W, C, 2] * [B, H, W, 1, 1]\n",
    "\n",
    "        if return_attn:\n",
    "            return output, attn_map\n",
    "        else:\n",
    "            return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0e72c78-a715-4766-965c-a31c39bcfb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Conv1D, Conv2DTranspose\n",
    "import math\n",
    "\n",
    "class MPCA(Layer):\n",
    "    def __init__(self, input_channel1=128, input_channel2=256, gamma=2, bias=1):\n",
    "        super(MPCA, self).__init__()\n",
    "        self.input_channel1 = input_channel1\n",
    "        self.input_channel2 = input_channel2\n",
    "\n",
    "        # Adaptive average pooling equivalent: global avg pool spatial dims\n",
    "        # We'll do manually with reduce_mean over height & width\n",
    "\n",
    "        kernel_size1 = int(abs((math.log(input_channel1, 2) + bias) / gamma))\n",
    "        kernel_size1 = kernel_size1 if kernel_size1 % 2 else kernel_size1 + 1\n",
    "\n",
    "        kernel_size2 = int(abs((math.log(input_channel2, 2) + bias) / gamma))\n",
    "        kernel_size2 = kernel_size2 if kernel_size2 % 2 else kernel_size2 + 1\n",
    "\n",
    "        kernel_size3 = int(abs((math.log(input_channel1 + input_channel2, 2) + bias) / gamma))\n",
    "        kernel_size3 = kernel_size3 if kernel_size3 % 2 else kernel_size3 + 1\n",
    "\n",
    "        # Conv1D layers - note input shape: (batch, steps, channels)\n",
    "        self.conv1 = Conv1D(filters=1, kernel_size=kernel_size1, padding='same', use_bias=False)\n",
    "        self.conv2 = Conv1D(filters=1, kernel_size=kernel_size2, padding='same', use_bias=False)\n",
    "        self.conv3 = Conv1D(filters=1, kernel_size=kernel_size3, padding='same', use_bias=False)\n",
    "\n",
    "        self.sigmoid = tf.keras.activations.sigmoid\n",
    "\n",
    "        self.up = Conv2DTranspose(filters=input_channel1,\n",
    "                                  kernel_size=3,\n",
    "                                  strides=2,\n",
    "                                  padding='same',\n",
    "                                  output_padding=1)\n",
    "    def call(self, x1, x2):\n",
    "        # x1, x2: [B, H, W, C]\n",
    "    \n",
    "        # Global avg pool spatial dims\n",
    "        a1 = tf.reduce_mean(x1, axis=[1, 2], keepdims=True)  # [B,1,1,C1]\n",
    "        a2 = tf.reduce_mean(x2, axis=[1, 2], keepdims=True)  # [B,1,1,C2]\n",
    "        # print(\"a1 image shape:\", a1.shape)\n",
    "        # print(\"a2 image shape:\", a2.shape)\n",
    "\n",
    "\n",
    "        # Squeeze height and width dims\n",
    "        a1 = tf.squeeze(a1, axis=[1, 2])  # [B, C1]\n",
    "        a2 = tf.squeeze(a2, axis=[1, 2])  # [B, C2]\n",
    "        # print(\"a1 after squueze image shape:\", a1.shape)\n",
    "        # print(\"a2 after squueze image shape:\", a2.shape)\n",
    "\n",
    "\n",
    "    \n",
    "        # Add a channel dim at the end for Conv1D: [B, length, channels=1]\n",
    "        a1 = tf.expand_dims(a1, axis=-1)  # [B, C1, 1]\n",
    "        a2 = tf.expand_dims(a2, axis=-1)  # [B, C2, 1]\n",
    "\n",
    "        # print(\"a1 after expand image shape:\", a1.shape)\n",
    "        # print(\"a2 after expand image shape:\", a2.shape)\n",
    "\n",
    "        # Conv1D along length (channel dim from original tensor)\n",
    "        a1 = self.conv1(a1)  # [B, C1, 1]\n",
    "        a2 = self.conv2(a2)  # [B, C2, 1]\n",
    "        # print(\"a1 after conv1d image shape:\", a1.shape)\n",
    "        # print(\"a2 after conv1d image shape:\", a2.shape)\n",
    "    \n",
    "        # Concatenate along length dim (axis=1)\n",
    "        mid = tf.concat([a1, a2], axis=1)  # [B, C1 + C2, 1]\n",
    "        # print(\"mid after concat a1 and a2 image shape:\", mid.shape)\n",
    "    \n",
    "    \n",
    "        # Conv3 along length axis with channels=1\n",
    "        mid = self.conv3(mid)  # [B, C1 + C2, 1]\n",
    "        # print(\"mid after conv a1 and a2 image shape:\", mid.shape)\n",
    "    \n",
    "    \n",
    "        # Apply sigmoid activation\n",
    "        mid = self.sigmoid(mid)  # [B, C1 + C2, 1]\n",
    "        \n",
    "    \n",
    "        # Remove channel dim, shape: [B, C1 + C2]\n",
    "        mid = tf.squeeze(mid, axis=-1)\n",
    "        # print(\"mid after squeeze a1 and a2 image shape:\", mid.shape)\n",
    "    \n",
    "    \n",
    "        # Split back into attn1 and attn2\n",
    "        attn1, attn2 = tf.split(mid, [self.input_channel1, self.input_channel2], axis=1)  # both [B, C]\n",
    "        # print(\"attn1 image shape:\", attn1.shape)\n",
    "        # print(\"attn2 image shape:\", attn2.shape)\n",
    "    \n",
    "        # Reshape for broadcasting to [B,1,1,C]\n",
    "        attn1 = tf.reshape(attn1, [-1, 1, 1, self.input_channel1])\n",
    "        attn2 = tf.reshape(attn2, [-1, 1, 1, self.input_channel2])\n",
    "        # print(\"attn1 after resshape image shape:\", attn1.shape)\n",
    "        # print(\"attn2 after reshape image shape:\", attn2.shape)\n",
    "    \n",
    "        \n",
    "        # Apply attention weights\n",
    "        x1_out = x1 * attn1\n",
    "        x2_out = x2 * attn2\n",
    "        # print(\"x1_out image shape:\", x1_out.shape)\n",
    "        # print(\"x2_out image shape:\", x2_out.shape)\n",
    "    \n",
    "        \n",
    "    \n",
    "        # Upsample x2_out to match x1_out spatial dims\n",
    "        x2_out = self.up(x2_out)\n",
    "        # print(\"x2_out image upsanple shape:\", x2_out.shape)\n",
    "    \n",
    "        \n",
    "    \n",
    "        # Sum outputs\n",
    "        result = x1_out + x2_out\n",
    "        # print(\"result MPCA shape:\", result.shape)\n",
    "    \n",
    "        return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f616e04-5902-4c4a-9538-147730d50040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, Lambda, Add, LeakyReLU,  \\\n",
    "                                    MaxPooling2D, concatenate, UpSampling2D,\\\n",
    "                                    Multiply, ZeroPadding2D, Cropping2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9eb834c-8153-4ede-9918-a5f1cd5e32e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"C:\\\\Users\\\\DU\\\\aman_fastmri\\\\layer.ipynb\"\n",
    "    \n",
    "%run \"C:\\\\Users\\\\DU\\\\aman_fastmri\\\\activation.ipynb\"\n",
    "    \n",
    "%run \"C:\\\\Users\\\\DU\\\\aman_fastmri\\\\fft.ipynb\"\n",
    "#%run ./Modules/fft.ipynb\n",
    "\n",
    "# %run layer.ipynb\n",
    "# %run activation.ipynb\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bab4f92f-d6ee-4ecc-9c0f-84ed718cab49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "class ComplexEncoderTF(Model):\n",
    "    def __init__(self, input_shape=(320, 320, 2)):\n",
    "        super(ComplexEncoderTF, self).__init__()\n",
    "\n",
    "        # Block 1\n",
    "        self.block1_conv1 = complex_Conv2D(64, kernel_size=3, padding='same')\n",
    "        self.block1_conv2 = complex_Conv2D(64, kernel_size=3, padding='same')\n",
    "        self.pool1 = ComplexMaxPool2D_mag(pool_size=(2, 2))\n",
    "\n",
    "        # Block 2\n",
    "        self.block2_conv1 = complex_Conv2D(128, kernel_size=3, padding='same')\n",
    "        self.block2_conv2 = complex_Conv2D(128, kernel_size=3, padding='same')\n",
    "        self.pool2 = ComplexMaxPool2D_mag(pool_size=(2, 2))\n",
    "\n",
    "        # Block 3\n",
    "        self.block3_conv1 = complex_Conv2D(256, kernel_size=3, padding='same')\n",
    "        self.block3_conv2 = complex_Conv2D(256, kernel_size=3, padding='same')\n",
    "        self.pool3 = ComplexMaxPool2D_mag(pool_size=(2, 2))\n",
    "\n",
    "        # Block 4\n",
    "        self.block4_conv1 = complex_Conv2D(512, kernel_size=3, padding='same')\n",
    "        self.block4_conv2 = complex_Conv2D(512, kernel_size=3, padding='same')\n",
    "        self.pool4 = ComplexMaxPool2D_mag(pool_size=(2, 2))\n",
    "\n",
    "        # Block 5 (no pooling)\n",
    "        self.block5_conv1 = complex_Conv2D(512, kernel_size=3, padding='same')\n",
    "        self.block5_conv2 = complex_Conv2D(512, kernel_size=3, padding='same')\n",
    "\n",
    "        self.build((None, *input_shape))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        real = tf.expand_dims(inputs[..., 0], axis=-1)\n",
    "        imag = tf.expand_dims(inputs[..., 1], axis=-1)\n",
    "\n",
    "        # Block 1\n",
    "        r, i = self.block1_conv1(real, imag)\n",
    "        r, i = CReLU(r, i)\n",
    "        r, i = self.block1_conv2(r, i)\n",
    "        r, i = CReLU(r, i)\n",
    "        feat1 = (r, i)\n",
    "        r, i = self.pool1(r, i)\n",
    "\n",
    "        # Block 2\n",
    "        r, i = self.block2_conv1(r, i)\n",
    "        r, i = CReLU(r, i)\n",
    "        r, i = self.block2_conv2(r, i)\n",
    "        r, i = CReLU(r, i)\n",
    "        feat2 = (r, i)\n",
    "        r, i = self.pool2(r, i)\n",
    "\n",
    "        # Block 3\n",
    "        r, i = self.block3_conv1(r, i)\n",
    "        r, i = CReLU(r, i)\n",
    "        r, i = self.block3_conv2(r, i)\n",
    "        r, i = CReLU(r, i)\n",
    "        feat3 = (r, i)\n",
    "        r, i = self.pool3(r, i)\n",
    "\n",
    "        # Block 4\n",
    "        r, i = self.block4_conv1(r, i)\n",
    "        r, i = CReLU(r, i)\n",
    "        r, i = self.block4_conv2(r, i)\n",
    "        r, i = CReLU(r, i)\n",
    "        feat4 = (r, i)\n",
    "        r, i = self.pool4(r, i)\n",
    "\n",
    "        # Block 5 (no pooling)\n",
    "        r, i = self.block5_conv1(r, i)\n",
    "        r, i = CReLU(r, i)\n",
    "        r, i = self.block5_conv2(r, i)\n",
    "        r, i = CReLU(r, i)\n",
    "        feat5 = (r, i)\n",
    "\n",
    "        return feat1, feat2, feat3, feat4, feat5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ffe79af-1ed1-4a7a-8ded-d53176853fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FSA(tf.keras.layers.Layer):\n",
    "#     def __init__(self, ratio=10):\n",
    "#         super().__init__()\n",
    "#         self.agf = AdaptiveGlobalFilter(ratio=ratio)\n",
    "#         self.sa = SpatialAttention()\n",
    "\n",
    "#     def call(self, x):\n",
    "        \n",
    "#         \"\"\"\n",
    "#         x: Tensor of shape [B, H, W, C, 2]\n",
    "#         Returns:\n",
    "#             Tensor of shape [B, H, W, C, 2]\n",
    "#         \"\"\"\n",
    "    \n",
    "#         out1 = self.agf(x)  # Frequency attention\n",
    "        \n",
    "#         out2 = self.sa(x)   # Spatial attention\n",
    "#         out= out1 + out2  \n",
    "        \n",
    "#         return out  \n",
    "\n",
    "class FSA(tf.keras.layers.Layer):\n",
    "    def __init__(self, ratio=10):\n",
    "        super().__init__()\n",
    "        self.agf = AdaptiveGlobalFilter(ratio=ratio)\n",
    "        self.sa = SpatialAttention()\n",
    "\n",
    "\n",
    "    def call(self, x, return_attn=False):\n",
    "        out1 = self.agf(x)\n",
    "        if return_attn:\n",
    "            out2, attn_map = self.sa(x, return_attn=True)\n",
    "            out = out1 + out2\n",
    "            return out, attn_map\n",
    "        else:\n",
    "            out2 = self.sa(x)\n",
    "            out = out1 + out2\n",
    "            return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "706b8b7f-467f-41d1-a4c9-2679d488b1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipConnection(tf.keras.layers.Layer):\n",
    "    def __init__(self, ratio, input_channel1, input_channel2, size):\n",
    "        super().__init__()\n",
    "        self.mpca_real = MPCA(input_channel1, input_channel2)\n",
    "        self.mpca_imag = MPCA(input_channel1, input_channel2)\n",
    "        self.fsa = FSA(ratio=ratio)\n",
    "\n",
    "\n",
    "    def call(self, input1, input2):\n",
    "        real1, imag1 = input1\n",
    "        real2, imag2 = input2\n",
    "        out_real = self.mpca_real(real1, real2)\n",
    "        out_imag = self.mpca_imag(imag1, imag2)\n",
    "        # print(\"out_imag image shape:\", out_imag.shape)\n",
    "\n",
    "        # combined = tf.concat([tf.expand_dims(out_real, axis=-1),tf.expand_dims(out_imag, axis=-1)], axis=-1)  # shape: [B, H, W, 2]\n",
    "        # # Apply FSA\n",
    "        # # print(\"combined :\", combined.shape)\n",
    "        # output = self.fsa(combined)\n",
    "\n",
    "        # out_real = output[..., 0]\n",
    "        # out_imag = output[..., 1]\n",
    "\n",
    "        return out_real, out_imag\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# batch_size = 1\n",
    "# height, width = 64, 64\n",
    "# input_channel1 = 8\n",
    "# input_channel2 = 16\n",
    "# size = (64, 64)\n",
    "\n",
    "# real1 = tf.random.normal((batch_size, height, width, input_channel1))\n",
    "# imag1 = tf.random.normal((batch_size, height, width, input_channel1))\n",
    "# real2 = tf.random.normal((batch_size, 32, 32, input_channel2))\n",
    "# imag2 = tf.random.normal((batch_size, 32, 32, input_channel2))\n",
    "\n",
    "# # Create and run layer\n",
    "# skip = SkipConnection(ratio=4, input_channel1=input_channel1, input_channel2=input_channel2, size=size)\n",
    "# out_real, out_imag = skip((real1, imag1), (real2, imag2))\n",
    "\n",
    "# print(\"✅ Output Real Shape:\", out_real.shape)\n",
    "# print(\"✅ Output Imag Shape:\", out_imag.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6925e9b6-12cf-4af5-b7bd-7d750f8542b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexUnetUpTF(tf.keras.layers.Layer):\n",
    "    def __init__(self, out_channels):\n",
    "        super(ComplexUnetUpTF, self).__init__()\n",
    "        self.upsample = ComplexUpSampling2D(size=(2, 2), interpolation='bilinear')\n",
    "        self.out_channels = out_channels\n",
    "        self.conv1 = complex_Conv2D(out_channels, kernel_size=3, padding='same')\n",
    "        self.conv2 = complex_Conv2D(out_channels, kernel_size=3, padding='same')\n",
    "        \n",
    "        # self.dense_block = DensBlock(out_channels)\n",
    "\n",
    "    def call(self, real_input1, imag_input1, real_input2, imag_input2):\n",
    "        # Upsample\n",
    "        up_real, up_imag = self.upsample(real_input2, imag_input2)\n",
    "\n",
    "        # Concatenate with skip connection\n",
    "        concat_real, concat_imag = concatenate_with(real_input1, imag_input1, up_real, up_imag)\n",
    "\n",
    "        # Proper flow through both conv layers\n",
    "        out_real, out_imag = self.conv1(concat_real, concat_imag)\n",
    "        out_real, out_imag = self.conv2(out_real, out_imag)\n",
    "        out_real, out_imag = CReLU(out_real, out_imag)\n",
    "\n",
    "        return out_real, out_imag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94c1a2f3-9338-49b8-ae3e-34d6123eacb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "class SF_UNet_TF(Model):\n",
    "    def __init__(self, input_shape=(320, 320, 2), num_classes=2):\n",
    "        super().__init__()\n",
    "        self.encoder = ComplexEncoderTF(input_shape=input_shape)\n",
    "\n",
    "        self.skip4 = SkipConnection(ratio=10, input_channel1=256, input_channel2=256, size=40)\n",
    "        self.skip3 = SkipConnection(ratio=10, input_channel1=128, input_channel2=256, size=80)\n",
    "        self.skip2 = SkipConnection(ratio=10, input_channel1=64, input_channel2=128, size=160)\n",
    "        self.skip1 = SkipConnection(ratio=10, input_channel1=32, input_channel2=64, size=320)\n",
    "\n",
    "        self.unet_up4 = ComplexUnetUpTF(512)\n",
    "        self.unet_up3 = ComplexUnetUpTF(256)\n",
    "        self.unet_up2 = ComplexUnetUpTF(128)\n",
    "        self.unet_up1 = ComplexUnetUpTF(64)\n",
    "\n",
    "        self.final_conv = complex_Conv2D(filters=num_classes, kernel_size=1, activation='linear')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # real = tf.expand_dims(inputs[..., 0], axis=-1)\n",
    "        # imag = tf.expand_dims(inputs[..., 1], axis=-1)\n",
    "\n",
    "        feat1, feat2, feat3, feat4, feat5 = self.encoder(inputs)\n",
    "        # print(\"feat1 real shape:\", feat1[0].shape)\n",
    "        # print(\"feat2 real shape:\", feat2[0].shape)\n",
    "        # print(\"feat3 real shape:\", feat3[0].shape)\n",
    "        # print(\"feat4 real shape:\", feat4[0].shape)\n",
    "        # print(\"feat5 bottleneck real shape:\", feat5[0].shape)\n",
    "\n",
    "        # print(\"feat4 real shape:\", feat4[0].shape)\n",
    "        # print(\"feat4 imag shape:\", feat4[1].shape)\n",
    "        # print(\"feat5 real shape:\", feat5[0].shape)\n",
    "        # print(\"feat5 imag shape:\", feat5[1].shape)\n",
    "\n",
    "        # Skip + up stage 4\n",
    "        skip4_out_r, skip4_out_i = self.skip4(feat4, feat5)\n",
    "        # tf.print(\"Skip4 output shape (real):\", tf.shape(skip4_out_r))\n",
    "        # tf.print(\"Skip4 output shape (imag):\", tf.shape(skip4_out_i))\n",
    "\n",
    "        decoder4 = self.unet_up4(skip4_out_r, skip4_out_i, feat5[0], feat5[1])\n",
    "        # tf.print(\"UnetUp4 output shape (real):\", tf.shape(decoder4[0]))\n",
    "        # tf.print(\"UnetUp4 output shape (imag):\", tf.shape(decoder4[1]))\n",
    "\n",
    "        # Skip + up stage 3\n",
    "        skip3_out_r, skip3_out_i = self.skip3(feat3, decoder4)\n",
    "        # tf.print(\"Skip3 output shape (real):\", tf.shape(skip3_out_r))\n",
    "        # tf.print(\"Skip3 output shape (imag):\", tf.shape(skip3_out_i))\n",
    "\n",
    "        decoder3 = self.unet_up3(skip3_out_r, skip3_out_i, decoder4[0], decoder4[1])\n",
    "        # tf.print(\"UnetUp3 output shape (real):\", tf.shape(decoder3[0]))\n",
    "        # tf.print(\"UnetUp3 output shape (imag):\", tf.shape(decoder3[1]))\n",
    "\n",
    "        # Skip + up stage 2\n",
    "        skip2_out_r, skip2_out_i = self.skip2(feat2, decoder3)\n",
    "        # tf.print(\"Skip2 output shape (real):\", tf.shape(skip2_out_r))\n",
    "        # tf.print(\"Skip2 output shape (imag):\", tf.shape(skip2_out_i))\n",
    "\n",
    "        decoder2 = self.unet_up2(skip2_out_r, skip2_out_i, decoder3[0], decoder3[1])\n",
    "        # tf.print(\"UnetUp2 output shape (real):\", tf.shape(decoder2[0]))\n",
    "        # tf.print(\"UnetUp2 output shape (imag):\", tf.shape(decoder2[1]))\n",
    "\n",
    "        # Skip + up stage 1\n",
    "        skip1_out_r, skip1_out_i = self.skip1(feat1, decoder2)\n",
    "        # tf.print(\"Skip1 output shape (real):\", tf.shape(skip1_out_r))\n",
    "        # tf.print(\"Skip1 output shape (imag):\", tf.shape(skip1_out_i))\n",
    "\n",
    "        decoder1 = self.unet_up1(skip1_out_r, skip1_out_i, decoder2[0], decoder2[1])\n",
    "        # tf.print(\"UnetUp1 output shape (real):\", tf.shape(decoder1[0]))\n",
    "        # tf.print(\"UnetUp1 output shape (imag):\", tf.shape(decoder1[1]))\n",
    "\n",
    "        # Final 1×1 conv\n",
    "        real_out, imag_out = self.final_conv(decoder1[0], decoder1[1])\n",
    "        # tf.print(\"Final conv output shape (real):\", tf.shape(real_out))\n",
    "        # tf.print(\"Final conv output shape (imag):\", tf.shape(imag_out))\n",
    "\n",
    "        # Concatenate back to a 2-channel or 4-channel tensor\n",
    "        output = concatenate([real_out, imag_out], axis=-1)\n",
    "        # tf.print(\"Final concatenated output shape:\", tf.shape(output))\n",
    "\n",
    "        return output\n",
    "\n",
    "    # def call(self, inputs):\n",
    "    #     # Encoder\n",
    "    #     feat1, feat2, feat3, feat4, feat5 = self.encoder(inputs)\n",
    "    \n",
    "    #     # Decoder stage 4\n",
    "    #     skip4_r, skip4_i = self.skip4(feat4, feat5)\n",
    "    #     decoder4 = self.unet_up4(skip4_r, skip4_i, feat5[0], feat5[1])\n",
    "    \n",
    "    #     # Decoder stage 3\n",
    "    #     skip3_r, skip3_i = self.skip3(feat3, feat4)\n",
    "    #     decoder3 = self.unet_up3(skip3_r, skip3_i, decoder4[0], decoder4[1])\n",
    "    \n",
    "    #     # Decoder stage 2\n",
    "    #     skip2_r, skip2_i = self.skip2(feat2, feat3)\n",
    "    #     decoder2 = self.unet_up2(skip2_r, skip2_i, decoder3[0], decoder3[1])\n",
    "    \n",
    "    #     # Decoder stage 1\n",
    "    #     skip1_r, skip1_i = self.skip1(feat1, feat2)\n",
    "    #     decoder1 = self.unet_up1(skip1_r, skip1_i, decoder2[0], decoder2[1])\n",
    "    \n",
    "    #     # Final 1x1 convolution\n",
    "    #     real_out, imag_out = self.final_conv(decoder1[0], decoder1[1])\n",
    "    \n",
    "    #     # Concatenate along last dimension: (B, H, W, C) -> (B, H, W, 2C)\n",
    "    #     output = concatenate([real_out, imag_out], axis=-1)\n",
    "    \n",
    "    #     return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6ca92fb-6342-4a8a-83fd-c4a12deb7def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat1 real shape: (None, 320, 320, 32)\n",
      "feat2 real shape: (None, 160, 160, 64)\n",
      "feat3 real shape: (None, 80, 80, 128)\n",
      "feat4 real shape: (None, 40, 40, 256)\n",
      "feat5 bottleneck real shape: (None, 20, 20, 256)\n",
      "feat4 real shape: (None, 40, 40, 256)\n",
      "feat4 imag shape: (None, 40, 40, 256)\n",
      "feat5 real shape: (None, 20, 20, 256)\n",
      "feat5 imag shape: (None, 20, 20, 256)\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 320, 320, 2)]     0         \n",
      "                                                                 \n",
      " sf_u_net_tf_3 (SF_UNet_TF)  (None, 320, 320, 2)       24172146  \n",
      "                                                                 \n",
      " lambda_2 (Lambda)           (None, 320, 320, 2)       0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,172,146\n",
      "Trainable params: 24,036,146\n",
      "Non-trainable params: 136,000\n",
      "_________________________________________________________________\n",
      "feat1 real shape: (2, 320, 320, 32)\n",
      "feat2 real shape: (2, 160, 160, 64)\n",
      "feat3 real shape: (2, 80, 80, 128)\n",
      "feat4 real shape: (2, 40, 40, 256)\n",
      "feat5 bottleneck real shape: (2, 20, 20, 256)\n",
      "feat4 real shape: (2, 40, 40, 256)\n",
      "feat4 imag shape: (2, 40, 40, 256)\n",
      "feat5 real shape: (2, 20, 20, 256)\n",
      "feat5 imag shape: (2, 20, 20, 256)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "def build_dual_output_model(kshape=(3, 3), H=320, W=320, channels=2):\n",
    "    input_img = Input(shape=(H, W, channels))\n",
    "\n",
    "    # Image-domain SF_UNet_TF only\n",
    "    unet = SF_UNet_TF()\n",
    "    output_img = unet(input_img)\n",
    "\n",
    "    # Assign a name to the output for use in loss dict\n",
    "    output_img = Lambda(lambda x: x)(output_img)\n",
    "\n",
    "    model = Model(\n",
    "        inputs=input_img,\n",
    "        outputs=output_img\n",
    "    )\n",
    "\n",
    "    return model\n",
    "model = build_dual_output_model(H=320, W=320)\n",
    "model.summary()\n",
    "\n",
    "# Create a dummy batch: batch=2, H=64, W=64, 2 channels (real+imag)\n",
    "dummy = np.random.randn(2, 320, 320, 2).astype(np.float32)\n",
    "\n",
    "# Forward-pass: this will trigger our tf.print in SkipConnection.call\n",
    "_ = model(dummy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f3e5efe-4d91-41b7-9a0e-b72c5ae24378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.losses import MeanSquaredError\n",
    "\n",
    "# # Build the model\n",
    "# model = build_dual_output_model(H=320, W=320, channels=2)\n",
    "\n",
    "# # Create dummy input and target tensors\n",
    "# x = tf.random.normal([1, 320, 320, 2])  # input: real + imag\n",
    "# y_true = tf.random.normal([1, 320, 320, 2])  # target: real + imag\n",
    "\n",
    "# # Use Mean Squared Error as loss\n",
    "# loss_fn = MeanSquaredError()\n",
    "\n",
    "# # Track gradients\n",
    "# with tf.GradientTape() as tape:\n",
    "#     y_pred = model(x)\n",
    "#     loss = loss_fn(y_true, y_pred)\n",
    "\n",
    "# # Get trainable variables and compute gradients\n",
    "# trainable_vars = model.trainable_variables\n",
    "# grads = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "# # Check which gradients are None\n",
    "# none_grads = [var.name for var, g in zip(trainable_vars, grads) if g is None]\n",
    "\n",
    "# # Display result\n",
    "# if none_grads:\n",
    "#     print(\"⚠️ Gradients are NOT flowing for these variables:\")\n",
    "#     for name in none_grads:\n",
    "#         print(f\" - {name}\")\n",
    "# else:\n",
    "#     print(\"✅ All gradients are flowing correctly!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0757837-86d4-4471-93ef-6cc48d12bd49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfdc848-9483-4973-a9eb-9b1c91903626",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
