{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b37b81cc-8826-42d6-8aeb-c5dfc9fd45f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fbcbacb-db3c-4574-b17c-0c06b5e0e52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = np.load('./Data/Data/mask_4x_320_random.npy')  # Shape: (1, 320, 320)\n",
    "\n",
    "# print(\"og shape:\", mask.shape)\n",
    "\n",
    "# # Use np.tile to reshape it to (1, 320, 320, 1)\n",
    "# # var_sampling_mask = np.tile(var_sampling_mask[..., np.newaxis], (1, 1, 1, 1))  # Final shape: (1, 320, 320, 1)\n",
    "# mask = np.tile(mask, (1, 320, 1, 2))  # tile height=320 times\n",
    "\n",
    "# # Confirm final shape\n",
    "# print(\"New shape:\", mask.shape) \n",
    "# mask_for_plot = np.squeeze(mask[...,0])  # Shape: (320, 320)\n",
    "\n",
    "# # Plot\n",
    "# plt.figure(figsize=(5, 5))\n",
    "# plt.imshow(mask_for_plot, cmap='gray')\n",
    "# plt.title(\"Tiled Sampling Mask (320x320)\")\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d85f7bea-46a9-423e-8ff8-44eee447c5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class MRISliceGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, file_list, batch_size=4, shuffle=True, mask=None):\n",
    "        self.file_list = file_list\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.mask = mask  # Shape: (1, 320, 320, 2)\n",
    "        self.slice_index_map = []\n",
    "        self._build_index()\n",
    "\n",
    "    def _build_index(self):\n",
    "        for file_idx, file_path in enumerate(self.file_list):\n",
    "            with h5py.File(file_path, 'r') as f:\n",
    "                num_slices = f['image_under'].shape[0]\n",
    "                for slice_idx in range(num_slices):\n",
    "                    self.slice_index_map.append((file_idx, slice_idx))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.slice_index_map) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_map = self.slice_index_map[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        input_img_batch = []\n",
    "        target_img_batch = []\n",
    "        input_kspace_batch = []\n",
    "\n",
    "        for file_idx, slice_idx in batch_map:\n",
    "            with h5py.File(self.file_list[file_idx], 'r') as f:\n",
    "                input_img = f['image_under'][slice_idx]       # shape: [H, W, 2]\n",
    "                target_img = f['image_full'][slice_idx]       # shape: [H, W, 2]\n",
    "                input_kspace = f['kspace_under'][slice_idx]   # shape: [H, W, 2]\n",
    "\n",
    "                input_img_batch.append(input_img)\n",
    "                target_img_batch.append(target_img)\n",
    "                input_kspace_batch.append(input_kspace)\n",
    "\n",
    "        x_img = np.stack(input_img_batch, axis=0)\n",
    "        x_kspace = np.stack(input_kspace_batch, axis=0)\n",
    "        y_batch = np.stack(target_img_batch, axis=0)\n",
    "\n",
    "        # if self.mask is not None:\n",
    "        #     actual_batch_size = len(x_img)\n",
    "        #     if self.mask.shape == (1, 320, 320, 2):\n",
    "        #         mask_batch = np.tile(self.mask, (actual_batch_size, 1, 1, 1))\n",
    "        #     else:\n",
    "        #         raise ValueError(\"Mask must have shape (1, 320, 320, 2)\")\n",
    "        #     return [x_img, mask_batch, x_kspace], y_batch\n",
    "        # else:\n",
    "        #     return [x_img, x_kspace], y_batch\n",
    "        return x_img, y_batch\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.slice_index_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8a39dc6-ccc4-40f7-afb7-2cdbec7f134b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = r\"D:\\fastmri_single_coil_FSSCAN_4x\\train_sub_norm\"\n",
    "val_folder = r\"D:\\fastmri_single_coil_FSSCAN_4x\\val_sub_norm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a7e64d3-bb0b-413a-9283-d78c7a9a6890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "kspace_files_list_train = sorted(glob.glob(os.path.join(train_folder, \"*.h5\")))\n",
    "kspace_files_list_val = sorted(glob.glob(os.path.join(val_folder, \"*.h5\")))\n",
    "\n",
    "# half_train = 20\n",
    "# half_val = 10\n",
    "half_train = len(kspace_files_list_train) \n",
    "half_val = len(kspace_files_list_val) \n",
    "# print(\"half_train\",half_train)\n",
    "# print(\"half_val\",half_val)\n",
    "kspace_files_list_train = kspace_files_list_train[:half_train]\n",
    "kspace_files_list_val = kspace_files_list_val[:half_val]\n",
    "\n",
    "# Create generators\n",
    "# train_gen = MRISliceGenerator(kspace_files_list_train,batch_size=16, shuffle=True,mask=mask)\n",
    "# val_gen = MRISliceGenerator(kspace_files_list_val, batch_size=4, shuffle=False,mask=mask)\n",
    "train_gen = MRISliceGenerator(kspace_files_list_train,batch_size=16, shuffle=True)\n",
    "val_gen = MRISliceGenerator(kspace_files_list_val, batch_size=4, shuffle=False)\n",
    "\n",
    "print(len(train_gen))  \n",
    "print(len(val_gen))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ff52e5a-6280-4914-9d3e-4b25c343bcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 320, 320, 2)]     0         \n",
      "                                                                 \n",
      " sf_u_net_tf (SF_UNet_TF)    (None, 320, 320, 2)       22217290  \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 320, 320, 2)       0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,217,290\n",
      "Trainable params: 22,081,290\n",
      "Non-trainable params: 136,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "%run \"C:\\\\Users\\\\DU\\\\aman_fastmri\\\\ablation_model_FSA_Channel_Pooling.ipynb\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c98876e-c6b9-4480-bbaa-b77cbc133e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import Loss\n",
    "\n",
    "class PerpLoss(tf.keras.losses.Loss):\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Split into real and imaginary components\n",
    "        y_true_real, y_true_imag = y_true[..., 0], y_true[..., 1]\n",
    "        y_pred_real, y_pred_imag = y_pred[..., 0], y_pred[..., 1]\n",
    "\n",
    "        # Reconstruct complex tensors\n",
    "        y_true_complex = tf.complex(y_true_real, y_true_imag)\n",
    "        y_pred_complex = tf.complex(y_pred_real, y_pred_imag)\n",
    "\n",
    "        # Magnitudes\n",
    "        mag_pred = tf.abs(y_pred_complex)\n",
    "        mag_target = tf.abs(y_true_complex)\n",
    "\n",
    "        # Cross product magnitude\n",
    "        cross = tf.abs(y_true_real * y_pred_imag - y_true_imag * y_pred_real)\n",
    "\n",
    "        # Angle difference\n",
    "        angle_true = tf.math.atan2(y_true_imag, y_true_real)\n",
    "        angle_pred = tf.math.atan2(y_pred_imag, y_pred_real)\n",
    "        angle_diff = angle_true - angle_pred\n",
    "\n",
    "        # perp loss part\n",
    "        ploss = cross / (mag_pred + 1e-8)\n",
    "        phase = tf.math.cos(angle_diff)\n",
    "        aligned_mask = tf.math.less(phase, 0.0)\n",
    "\n",
    "        final_term = tf.where(aligned_mask,\n",
    "                              mag_target + (mag_target - ploss),\n",
    "                              ploss)\n",
    "\n",
    "        # Combine with magnitude MSE\n",
    "        mse_mag = tf.reduce_mean(tf.square(mag_pred - mag_target))\n",
    "        total_loss = tf.reduce_mean(final_term + mse_mag)\n",
    "        return total_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81921b93-2cd2-441b-8e05-56495c28e398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸ”§ TRAINING CONFIGURATION\n",
      "============================================================\n",
      " Save Directory:       ./SavedModels_ablation\n",
      " Model Dimensions:     320x320\n",
      " Epochs:               20\n",
      " Learning Rate:        0.0001\n",
      " Checkpoint Weights:   ./SavedModels_ablation\\model_FSA_channel_pooling.h5\n",
      " Final Weights Path:   ./SavedModels_ablation\\model_FSA_channel_pooling.h5\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# --- Directory Setup ---\n",
    "save_dir = \"./SavedModels_ablation\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# --- Configuration ---\n",
    "H, W        = 320, 320\n",
    "EPOCHS      = 20\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# define checkpoint & final weight filepaths (use .h5 for weights-only)\n",
    "MODEL_NAME   = os.path.join(save_dir, \"model_FSA_channel_pooling.h5\")\n",
    "WEIGHTS_FINAL = os.path.join(save_dir, \"model_FSA_channel_pooling.h5\")\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ðŸ”§ TRAINING CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\" Save Directory:       {save_dir}\")\n",
    "print(f\" Model Dimensions:     {H}x{W}\")\n",
    "print(f\" Epochs:               {EPOCHS}\")\n",
    "print(f\" Learning Rate:        {LEARNING_RATE}\")\n",
    "print(f\" Checkpoint Weights:   {MODEL_NAME}\")\n",
    "print(f\" Final Weights Path:   {WEIGHTS_FINAL}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# --- Model Setup ---\n",
    "model = build_dual_output_model()\n",
    "\n",
    "# --- Optimizer & Compile ---\n",
    "optimizer = Adam(learning_rate=LEARNING_RATE)\n",
    "model.compile(optimizer=optimizer, loss=PerpLoss())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64cf6b11-c530-4380-8e5d-f00fa333be3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint weights from ./SavedModels_ablation\\model_FSA_channel_pooling.h5\n",
      "\n",
      "ðŸš€ STARTING TRAINING...\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to stack",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 46\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸš€ STARTING TRAINING...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m---> 46\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m     52\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m TRAINING COMPLETED\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# --- Save Final Weights ---\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\WNet\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[3], line 42\u001b[0m, in \u001b[0;36mMRISliceGenerator.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     39\u001b[0m         target_img_batch\u001b[38;5;241m.\u001b[39mappend(target_img)\n\u001b[0;32m     40\u001b[0m         input_kspace_batch\u001b[38;5;241m.\u001b[39mappend(input_kspace)\n\u001b[1;32m---> 42\u001b[0m x_img \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_img_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m x_kspace \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(input_kspace_batch, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     44\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(target_img_batch, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\WNet\\lib\\site-packages\\numpy\\core\\shape_base.py:445\u001b[0m, in \u001b[0;36mstack\u001b[1;34m(arrays, axis, out, dtype, casting)\u001b[0m\n\u001b[0;32m    443\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [asanyarray(arr) \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m arrays:\n\u001b[1;32m--> 445\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneed at least one array to stack\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    447\u001b[0m shapes \u001b[38;5;241m=\u001b[39m {arr\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays}\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shapes) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: need at least one array to stack"
     ]
    }
   ],
   "source": [
    "# # --- Optimizer & Compile ---\n",
    "# optimizer = Adam(learning_rate=LEARNING_RATE)\n",
    "# model.compile(optimizer=optimizer, loss=PerpLoss())\n",
    "\n",
    "# --- Load Existing Best Weights ---\n",
    "if os.path.isfile(MODEL_NAME):\n",
    "    try:\n",
    "        model.load_weights(MODEL_NAME)\n",
    "        print(f\"Loaded checkpoint weights from {MODEL_NAME}\")\n",
    "    except Exception as e:\n",
    "        print(f\" Could not load checkpoint: {e}\\n   Starting from scratch.\")\n",
    "else:\n",
    "    print(\"No existing checkpoint found. Starting from scratch.\")\n",
    "\n",
    "# --- Callbacks ---\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    filepath=MODEL_NAME,\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")\n",
    "\n",
    "earlystop_cb = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=20,\n",
    "    verbose=1,\n",
    "    mode=\"min\",\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr_cb = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.5,\n",
    "    patience=10,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1,\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "callbacks = [checkpoint_cb, earlystop_cb, reduce_lr_cb]\n",
    "\n",
    "# --- Training ---\n",
    "print(\"\\nðŸš€ STARTING TRAINING...\")\n",
    "print(\"=\" * 60)\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "print(\"\\n TRAINING COMPLETED\")\n",
    "\n",
    "# --- Save Final Weights ---\n",
    "model.save_weights(WEIGHTS_FINAL)\n",
    "print(f\" Final weights saved to {WEIGHTS_FINAL}\")\n",
    "\n",
    "# --- Verify Checkpoints on Disk ---\n",
    "# print(\"\\n Contents of checkpoint directory:\")\n",
    "# for f in sorted(glob.glob(os.path.join(save_dir, \"*.h5\"))):\n",
    "#     print(\"   \", f)\n",
    "\n",
    "# --- Training Analysis ---\n",
    "if history:\n",
    "    print(\"\\n TRAINING ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    best_epoch     = np.argmin(history.history[\"val_loss\"]) + 1\n",
    "    best_val_loss  = np.min(history.history[\"val_loss\"])\n",
    "    final_train    = history.history[\"loss\"][-1]\n",
    "    final_val      = history.history[\"val_loss\"][-1]\n",
    "\n",
    "    print(f\" Best Epoch:           {best_epoch}\")\n",
    "    print(f\" Best Validation Loss: {best_val_loss:.6f}\")\n",
    "    print(f\" Final Training Loss:  {final_train:.6f}\")\n",
    "    print(f\" Final Validation Loss:{final_val:.6f}\")\n",
    "\n",
    "# --- Plot Training Curves ---\n",
    "def plot_training_history(hist):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(hist[\"loss\"],     label=\"Train Loss\", linewidth=2)\n",
    "    plt.plot(hist[\"val_loss\"], label=\"Val Loss\",   linewidth=2)\n",
    "    best_ep = int(np.argmin(hist[\"val_loss\"]))\n",
    "    plt.axvline(best_ep, linestyle=\"--\", label=f\"Best Epoch: {best_ep+1}\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training & Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if history:\n",
    "    plot_training_history(history.history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33053377-37f9-4738-9ff3-490f76532485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"ðŸ“‚ Current Working Directory:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dd22e4-0ca5-4ca7-bdd3-b682d00e8faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
    "\n",
    "# Path to validation folder\n",
    "# val_folder = \"F:/denoised_preprocessed_h5_val\"\n",
    "\n",
    "# val_folder = r\"E:\\fastmri\\val_norm\"\n",
    "# val_folder = r\"D:\\val_norm\"\n",
    "\n",
    "# val_folder = r\"G:\\val_norm\\val_norm\"\n",
    "# files = sorted([os.path.join(val_folder, f) for f in os.listdir(val_folder) if f.endswith(\".h5\")])\n",
    "kspace_files_list_val = sorted(glob.glob(os.path.join(val_folder, \"*.h5\")))\n",
    "file_paths = kspace_files_list_val\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# HELPERS\n",
    "# ----------------------\n",
    "def to_complex(x):\n",
    "    return x[..., 0] + 1j * x[..., 1]\n",
    "\n",
    "def nmse(gt, pred):\n",
    "    return np.linalg.norm(gt - pred) ** 2 / (np.linalg.norm(gt) ** 2 + 1e-10)\n",
    "\n",
    "def compute_ssim(gt, pred, max_val):\n",
    "    return structural_similarity(\n",
    "        gt, pred,\n",
    "        data_range=max_val,\n",
    "        win_size=9,\n",
    "        gaussian_weights=False,\n",
    "        use_sample_covariance=False,\n",
    "        K1=0.01,\n",
    "        K2=0.03\n",
    "    )\n",
    "\n",
    "# ----------------------\n",
    "# STORAGE\n",
    "# ----------------------\n",
    "ssim_list = []\n",
    "psnr_list = []\n",
    "nmse_list = []\n",
    "\n",
    "# ----------------------\n",
    "# PROCESSING\n",
    "# ----------------------\n",
    "for file in tqdm(file_paths, desc=\"Processing volumes\"):\n",
    "    with h5py.File(file, 'r') as f:\n",
    "        image_full = f[\"image_full\"][:]       # (slices, H, W, 2)\n",
    "        image_under = f[\"image_under\"][:]     # (slices, H, W, 2)\n",
    "        max_val = float(f[\"max_val_full_image\"][0])\n",
    "\n",
    "#     mask_batch = np.tile(mask, (image_under.shape[0], 1, 1, 1)) \n",
    "    # Get model prediction (still in normalized form)\n",
    "    # pred = model.predict([image_under,mask_batch,image_under], verbose=0)  # shape (slices, H, W, 2)\n",
    "    pred = model.predict(image_under, verbose=0)  # shape (slices, H, W, 2)\n",
    "    \n",
    "    image_full *= max_val\n",
    "    \n",
    "    pred *= max_val  # Scale predicted output to original intensity range\n",
    "\n",
    "    # Convert to complex and get magnitude\n",
    "    gt_mag = np.abs(to_complex(image_full))\n",
    "    pred_mag = np.abs(to_complex(pred))\n",
    "\n",
    "    # Volume-wise PSNR and NMSE\n",
    "    psnr_val = peak_signal_noise_ratio(gt_mag, pred_mag, data_range=max_val)\n",
    "    nmse_val = nmse(gt_mag.flatten(), pred_mag.flatten())\n",
    "\n",
    "    psnr_list.append(psnr_val)\n",
    "    nmse_list.append(nmse_val)\n",
    "\n",
    "    # Slice-wise SSIM\n",
    "    for i in range(gt_mag.shape[0]):\n",
    "        ssim_val = compute_ssim(gt_mag[i], pred_mag[i], max_val)\n",
    "        ssim_list.append(ssim_val)\n",
    "\n",
    "# ----------------------\n",
    "# REPORT\n",
    "# ----------------------\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(f\"PSNR (Mag, volume): {np.mean(psnr_list):.2f} Â± {np.std(psnr_list):.2f} dB\")\n",
    "print(f\"NMSE (Mag, volume): {np.mean(nmse_list):.6f} Â± {np.std(nmse_list):.6f}\")\n",
    "print(f\"SSIM (Mag, slice):  {np.mean(ssim_list):.4f} Â± {np.std(ssim_list):.4f}\")\n",
    "\n",
    "print(\"=\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73230c5f-508f-4e0e-8ef1-51b34cb3117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
    "\n",
    "# Path to validation folder\n",
    "# val_folder = \"F:/denoised_preprocessed_h5_val\"\n",
    "\n",
    "# val_folder = r\"E:\\fastmri\\val_norm\"\n",
    "# val_folder = r\"D:\\val_norm\"\n",
    "model.load_weights(\".\\SavedModels_ablation\\model_FSA_channel_pooling.h5\")\n",
    "\n",
    "\n",
    "# val_folder = r\"G:\\val_norm\\val_norm\"\n",
    "# files = sorted([os.path.join(val_folder, f) for f in os.listdir(val_folder) if f.endswith(\".h5\")])\n",
    "kspace_files_list_val = sorted(glob.glob(os.path.join(val_folder, \"*.h5\")))\n",
    "file_paths = kspace_files_list_val\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# HELPERS\n",
    "# ----------------------\n",
    "def to_complex(x):\n",
    "    return x[..., 0] + 1j * x[..., 1]\n",
    "\n",
    "def nmse(gt, pred):\n",
    "    return np.linalg.norm(gt - pred) ** 2 / (np.linalg.norm(gt) ** 2 + 1e-10)\n",
    "\n",
    "def compute_ssim(gt, pred, max_val):\n",
    "    return structural_similarity(\n",
    "        gt, pred,\n",
    "        data_range=max_val,\n",
    "        win_size=9,\n",
    "        gaussian_weights=False,\n",
    "        use_sample_covariance=False,\n",
    "        K1=0.01,\n",
    "        K2=0.03\n",
    "    )\n",
    "\n",
    "# ----------------------\n",
    "# STORAGE\n",
    "# ----------------------\n",
    "ssim_list = []\n",
    "psnr_list = []\n",
    "nmse_list = []\n",
    "\n",
    "# ----------------------\n",
    "# PROCESSING\n",
    "# ----------------------\n",
    "for file in tqdm(file_paths, desc=\"Processing volumes\"):\n",
    "    with h5py.File(file, 'r') as f:\n",
    "        image_full = f[\"image_full\"][:]       # (slices, H, W, 2)\n",
    "        image_under = f[\"image_under\"][:]     # (slices, H, W, 2)\n",
    "        max_val = float(f[\"max_val_full_image\"][0])\n",
    "\n",
    "#     mask_batch = np.tile(mask, (image_under.shape[0], 1, 1, 1)) \n",
    "    # Get model prediction (still in normalized form)\n",
    "    # pred = model.predict([image_under,mask_batch,image_under], verbose=0)  # shape (slices, H, W, 2)\n",
    "    #pred = model.predict(image_under, verbose=0)  # shape (slices, H, W, 2)\n",
    "    \n",
    "    image_full *= max_val\n",
    "    \n",
    "    image_under *= max_val  # Scale predicted output to original intensity range\n",
    "\n",
    "    # Convert to complex and get magnitude\n",
    "    gt_mag = np.abs(to_complex(image_full))\n",
    "    pred_mag = np.abs(to_complex(image_under))\n",
    "\n",
    "    # Volume-wise PSNR and NMSE\n",
    "    psnr_val = peak_signal_noise_ratio(gt_mag, pred_mag, data_range=max_val)\n",
    "    nmse_val = nmse(gt_mag.flatten(), pred_mag.flatten())\n",
    "\n",
    "    psnr_list.append(psnr_val)\n",
    "    nmse_list.append(nmse_val)\n",
    "\n",
    "    # Slice-wise SSIM\n",
    "    for i in range(gt_mag.shape[0]):\n",
    "        ssim_val = compute_ssim(gt_mag[i], pred_mag[i], max_val)\n",
    "        ssim_list.append(ssim_val)\n",
    "\n",
    "# ----------------------\n",
    "# REPORT\n",
    "# ----------------------\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(f\"PSNR (Mag, volume): {np.mean(psnr_list):.2f} Â± {np.std(psnr_list):.2f} dB\")\n",
    "print(f\"NMSE (Mag, volume): {np.mean(nmse_list):.6f} Â± {np.std(nmse_list):.6f}\")\n",
    "print(f\"SSIM (Mag, slice):  {np.mean(ssim_list):.4f} Â± {np.std(ssim_list):.4f}\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afc879f-9d60-4ce0-9156-79f44555cdb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
